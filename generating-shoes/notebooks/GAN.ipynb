{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"G3_cRswztOgg","colab_type":"code","outputId":"d1ea06d8-6686-48b0-ece4-173730f2ca79","executionInfo":{"status":"error","timestamp":1575997817685,"user_tz":-60,"elapsed":703498,"user":{"displayName":"Pratham Solanki","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAlo2-DSwveVStpx6EtTeKypxgtNWdHD_nOwwms=s64","userId":"15166068430047448392"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1yFD3UzfGACMOGd1AisKe38qYsn7LaIU6"}},"source":["import argparse\n","import os\n","import numpy as np\n","import math\n","\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torch.autograd import Variable\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","os.makedirs(\"images\", exist_ok=True)\n","\n","n_epochs = 200 # number of epochs of training\n","batch_size = 64 # size of the batches\n","lr = 0.0002 # adam: learning rate\n","b1 = 0.5 # adam: decay of first order momentum of gradient\n","b2 = 0.999 # adam: decay of first order momentum of gradient\n","latent_dim = 100 # dimensionality of the latent space\n","img_size = 28 # size of each image dimension\n","channels = 1 # number of image channels\n","sample_interval = 400 # interval betwen image samples\n","\n","img_shape = (channels, img_size, img_size)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        def block(in_feat, out_feat, normalize=True):\n","            layers = [nn.Linear(in_feat, out_feat)]\n","            if normalize:\n","                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n","            layers.append(nn.LeakyReLU(0.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *block(latent_dim, 128, normalize=False),\n","            *block(128, 256),\n","            *block(256, 512),\n","            *block(512, 1024),\n","            nn.Linear(1024, int(np.prod(img_shape))),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        img = img.view(img.size(0), *img_shape)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Linear(int(np.prod(img_shape)), 512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, img):\n","        img_flat = img.view(img.size(0), -1)\n","        validity = self.model(img_flat)\n","\n","        return validity\n","\n","\n","# Loss function\n","adversarial_loss = torch.nn.BCELoss()\n","\n","# Initialize generator and discriminator\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","generator.cuda()\n","discriminator.cuda()\n","adversarial_loss.cuda()\n","\n","# Configure data loader\n","os.makedirs(\"../../data/mnist\", exist_ok=True)\n","dataloader = torch.utils.data.DataLoader(\n","    datasets.MNIST(\n","        \"../../data/mnist\",\n","        train=True,\n","        download=True,\n","        transform=transforms.Compose(\n","            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n","        ),\n","    ),\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","\n","# Optimizers\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n","optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n","\n","Tensor = torch.cuda.FloatTensor\n","\n","# ----------\n","#  Training\n","# ----------\n","\n","for epoch in range(n_epochs):\n","    for i, (imgs, _) in enumerate(dataloader):\n","\n","        # Adversarial ground truths\n","        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n","        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n","\n","        # Configure input\n","        real_imgs = Variable(imgs.type(Tensor))\n","\n","        # -----------------\n","        #  Train Generator\n","        # -----------------\n","\n","        optimizer_G.zero_grad()\n","\n","        # Sample noise as generator input\n","        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n","\n","        # Generate a batch of images\n","        gen_imgs = generator(z)\n","\n","        # Loss measures generator's ability to fool the discriminator\n","        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # ---------------------\n","        #  Train Discriminator\n","        # ---------------------\n","\n","        optimizer_D.zero_grad()\n","\n","        # Measure discriminator's ability to classify real from generated samples\n","        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n","        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n","        d_loss = (real_loss + fake_loss) / 2\n","\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n","            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n","        )\n","\n","        batches_done = epoch * len(dataloader) + i\n","        if batches_done % sample_interval == 0:\n","            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}