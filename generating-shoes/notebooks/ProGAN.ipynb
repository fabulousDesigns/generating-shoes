{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ProGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mk1b7jUl62TJ","colab_type":"code","colab":{}},"source":["import os\n","import random\n","\n","from skimage import io\n","import cv2\n","import numpy as np\n","import PIL\n","from PIL import Image, ImageOps\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","\n","#from pro_gan_pytorch.DataTools import get_data_loader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBxiqnOtctIo","colab_type":"code","outputId":"1cdbd818-3226-4e71-8cb3-559ee05308ed","executionInfo":{"status":"ok","timestamp":1580923928204,"user_tz":-60,"elapsed":24305,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":120}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"27FeA5P6Wnv5","colab_type":"code","outputId":"a2aef54d-4088-4c30-e566-22d808451fd3","executionInfo":{"status":"ok","timestamp":1580923930151,"user_tz":-60,"elapsed":4627,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls \"/content/drive/My Drive/BDMA/BDRP_clone\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["generated_images  generated_images.zip\timages.zip  models\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C-UMM04068se","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9b959a34-f19b-4aac-c490-3ffd90694afd","executionInfo":{"status":"ok","timestamp":1580923937805,"user_tz":-60,"elapsed":10177,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}}},"source":["!cp 'drive/My Drive/BDMA/BDRP_clone/images.zip' .\n","!unzip images.zip\n","!mkdir images\n","!mv *.jpeg images/\n","!mkdir generated_images"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Archive:  images.zip\n","  inflating: .DS_Store               \n","  inflating: shoes_1.jpeg            \n","  inflating: shoes_10000.jpeg        \n","  inflating: shoes_1002.jpeg         \n","  inflating: shoes_1005.jpeg         \n","  inflating: shoes_1007.jpeg         \n","  inflating: shoes_1009.jpeg         \n","  inflating: shoes_1011.jpeg         \n","  inflating: shoes_1013.jpeg         \n","  inflating: shoes_1014.jpeg         \n","  inflating: shoes_1017.jpeg         \n","  inflating: shoes_102.jpeg          \n","  inflating: shoes_1029.jpeg         \n","  inflating: shoes_1030.jpeg         \n","  inflating: shoes_1035.jpeg         \n","  inflating: shoes_1043.jpeg         \n","  inflating: shoes_1056.jpeg         \n","  inflating: shoes_1058.jpeg         \n","  inflating: shoes_1061.jpeg         \n","  inflating: shoes_1062.jpeg         \n","  inflating: shoes_1068.jpeg         \n","  inflating: shoes_1075.jpeg         \n","  inflating: shoes_1078.jpeg         \n","  inflating: shoes_1084.jpeg         \n","  inflating: shoes_1090.jpeg         \n","  inflating: shoes_1091.jpeg         \n","  inflating: shoes_1093.jpeg         \n","  inflating: shoes_1096.jpeg         \n","  inflating: shoes_1097.jpeg         \n","  inflating: shoes_11.jpeg           \n","  inflating: shoes_1104.jpeg         \n","  inflating: shoes_1105.jpeg         \n","  inflating: shoes_1106.jpeg         \n","  inflating: shoes_1111.jpeg         \n","  inflating: shoes_1119.jpeg         \n","  inflating: shoes_112.jpeg          \n","  inflating: shoes_1124.jpeg         \n","  inflating: shoes_113.jpeg          \n","  inflating: shoes_1135.jpeg         \n","  inflating: shoes_1142.jpeg         \n","  inflating: shoes_1152.jpeg         \n","  inflating: shoes_1154.jpeg         \n","  inflating: shoes_1156.jpeg         \n","  inflating: shoes_1162.jpeg         \n","  inflating: shoes_1165.jpeg         \n","  inflating: shoes_1170.jpeg         \n","  inflating: shoes_1172.jpeg         \n","  inflating: shoes_1176.jpeg         \n","  inflating: shoes_1186.jpeg         \n","  inflating: shoes_119.jpeg          \n","  inflating: shoes_1192.jpeg         \n","  inflating: shoes_1204.jpeg         \n","  inflating: shoes_1210.jpeg         \n","  inflating: shoes_1220.jpeg         \n","  inflating: shoes_1222.jpeg         \n","  inflating: shoes_1224.jpeg         \n","  inflating: shoes_1227.jpeg         \n","  inflating: shoes_1232.jpeg         \n","  inflating: shoes_1237.jpeg         \n","  inflating: shoes_1239.jpeg         \n","  inflating: shoes_1244.jpeg         \n","  inflating: shoes_125.jpeg          \n","  inflating: shoes_1258.jpeg         \n","  inflating: shoes_127.jpeg          \n","  inflating: shoes_1270.jpeg         \n","  inflating: shoes_1274.jpeg         \n","  inflating: shoes_1280.jpeg         \n","  inflating: shoes_1282.jpeg         \n","  inflating: shoes_1286.jpeg         \n","  inflating: shoes_1290.jpeg         \n","  inflating: shoes_1297.jpeg         \n","  inflating: shoes_1300.jpeg         \n","  inflating: shoes_1302.jpeg         \n","  inflating: shoes_1304.jpeg         \n","  inflating: shoes_1308.jpeg         \n","  inflating: shoes_131.jpeg          \n","  inflating: shoes_1313.jpeg         \n","  inflating: shoes_1316.jpeg         \n","  inflating: shoes_1327.jpeg         \n","  inflating: shoes_1332.jpeg         \n","  inflating: shoes_1335.jpeg         \n","  inflating: shoes_1336.jpeg         \n","  inflating: shoes_1337.jpeg         \n","  inflating: shoes_134.jpeg          \n","  inflating: shoes_135.jpeg          \n","  inflating: shoes_1351.jpeg         \n","  inflating: shoes_1356.jpeg         \n","  inflating: shoes_1366.jpeg         \n","  inflating: shoes_1368.jpeg         \n","  inflating: shoes_1372.jpeg         \n","  inflating: shoes_1374.jpeg         \n","  inflating: shoes_1376.jpeg         \n","  inflating: shoes_1386.jpeg         \n","  inflating: shoes_1395.jpeg         \n","  inflating: shoes_1398.jpeg         \n","  inflating: shoes_14.jpeg           \n","  inflating: shoes_1401.jpeg         \n","  inflating: shoes_1407.jpeg         \n","  inflating: shoes_141.jpeg          \n","  inflating: shoes_142.jpeg          \n","  inflating: shoes_1428.jpeg         \n","  inflating: shoes_144.jpeg          \n","  inflating: shoes_1441.jpeg         \n","  inflating: shoes_1443.jpeg         \n","  inflating: shoes_1452.jpeg         \n","  inflating: shoes_1460.jpeg         \n","  inflating: shoes_1467.jpeg         \n","  inflating: shoes_1468.jpeg         \n","  inflating: shoes_1481.jpeg         \n","  inflating: shoes_1493.jpeg         \n","  inflating: shoes_1503.jpeg         \n","  inflating: shoes_1506.jpeg         \n","  inflating: shoes_1508.jpeg         \n","  inflating: shoes_1514.jpeg         \n","  inflating: shoes_152.jpeg          \n","  inflating: shoes_1522.jpeg         \n","  inflating: shoes_1526.jpeg         \n","  inflating: shoes_1530.jpeg         \n","  inflating: shoes_1534.jpeg         \n","  inflating: shoes_1536.jpeg         \n","  inflating: shoes_1544.jpeg         \n","  inflating: shoes_1550.jpeg         \n","  inflating: shoes_1551.jpeg         \n","  inflating: shoes_156.jpeg          \n","  inflating: shoes_1560.jpeg         \n","  inflating: shoes_1561.jpeg         \n","  inflating: shoes_1568.jpeg         \n","  inflating: shoes_1572.jpeg         \n","  inflating: shoes_158.jpeg          \n","  inflating: shoes_1580.jpeg         \n","  inflating: shoes_1583.jpeg         \n","  inflating: shoes_1592.jpeg         \n","  inflating: shoes_1594.jpeg         \n","  inflating: shoes_1605.jpeg         \n","  inflating: shoes_1608.jpeg         \n","  inflating: shoes_1617.jpeg         \n","  inflating: shoes_1620.jpeg         \n","  inflating: shoes_1622.jpeg         \n","  inflating: shoes_1627.jpeg         \n","  inflating: shoes_1628.jpeg         \n","  inflating: shoes_1646.jpeg         \n","  inflating: shoes_1660.jpeg         \n","  inflating: shoes_1669.jpeg         \n","  inflating: shoes_1676.jpeg         \n","  inflating: shoes_1685.jpeg         \n","  inflating: shoes_169.jpeg          \n","  inflating: shoes_1690.jpeg         \n","  inflating: shoes_1696.jpeg         \n","  inflating: shoes_1698.jpeg         \n","  inflating: shoes_1699.jpeg         \n","  inflating: shoes_1706.jpeg         \n","  inflating: shoes_1708.jpeg         \n","  inflating: shoes_1709.jpeg         \n","  inflating: shoes_1712.jpeg         \n","  inflating: shoes_1725.jpeg         \n","  inflating: shoes_1729.jpeg         \n","  inflating: shoes_1730.jpeg         \n","  inflating: shoes_1731.jpeg         \n","  inflating: shoes_1734.jpeg         \n","  inflating: shoes_1746.jpeg         \n","  inflating: shoes_1753.jpeg         \n","  inflating: shoes_1757.jpeg         \n","  inflating: shoes_1768.jpeg         \n","  inflating: shoes_177.jpeg          \n","  inflating: shoes_1771.jpeg         \n","  inflating: shoes_1772.jpeg         \n","  inflating: shoes_1776.jpeg         \n","  inflating: shoes_1782.jpeg         \n","  inflating: shoes_1786.jpeg         \n","  inflating: shoes_1793.jpeg         \n","  inflating: shoes_1795.jpeg         \n","  inflating: shoes_1797.jpeg         \n","  inflating: shoes_1798.jpeg         \n","  inflating: shoes_1803.jpeg         \n","  inflating: shoes_1812.jpeg         \n","  inflating: shoes_1814.jpeg         \n","  inflating: shoes_1820.jpeg         \n","  inflating: shoes_1825.jpeg         \n","  inflating: shoes_1826.jpeg         \n","  inflating: shoes_183.jpeg          \n","  inflating: shoes_1834.jpeg         \n","  inflating: shoes_184.jpeg          \n","  inflating: shoes_1850.jpeg         \n","  inflating: shoes_1853.jpeg         \n","  inflating: shoes_1861.jpeg         \n","  inflating: shoes_1871.jpeg         \n","  inflating: shoes_1880.jpeg         \n","  inflating: shoes_1882.jpeg         \n","  inflating: shoes_1884.jpeg         \n","  inflating: shoes_1887.jpeg         \n","  inflating: shoes_1891.jpeg         \n","  inflating: shoes_1897.jpeg         \n","  inflating: shoes_1909.jpeg         \n","  inflating: shoes_1910.jpeg         \n","  inflating: shoes_1911.jpeg         \n","  inflating: shoes_1915.jpeg         \n","  inflating: shoes_1916.jpeg         \n","  inflating: shoes_1920.jpeg         \n","  inflating: shoes_1927.jpeg         \n","  inflating: shoes_1930.jpeg         \n","  inflating: shoes_1934.jpeg         \n","  inflating: shoes_1940.jpeg         \n","  inflating: shoes_1946.jpeg         \n","  inflating: shoes_1947.jpeg         \n","  inflating: shoes_1955.jpeg         \n","  inflating: shoes_1958.jpeg         \n","  inflating: shoes_1962.jpeg         \n","  inflating: shoes_197.jpeg          \n","  inflating: shoes_1971.jpeg         \n","  inflating: shoes_1976.jpeg         \n","  inflating: shoes_1985.jpeg         \n","  inflating: shoes_1987.jpeg         \n","  inflating: shoes_199.jpeg          \n","  inflating: shoes_1994.jpeg         \n","  inflating: shoes_1997.jpeg         \n","  inflating: shoes_2008.jpeg         \n","  inflating: shoes_2013.jpeg         \n","  inflating: shoes_2014.jpeg         \n","  inflating: shoes_2016.jpeg         \n","  inflating: shoes_2017.jpeg         \n","  inflating: shoes_2029.jpeg         \n","  inflating: shoes_2040.jpeg         \n","  inflating: shoes_2041.jpeg         \n","  inflating: shoes_2050.jpeg         \n","  inflating: shoes_2051.jpeg         \n","  inflating: shoes_2058.jpeg         \n","  inflating: shoes_2059.jpeg         \n","  inflating: shoes_206.jpeg          \n","  inflating: shoes_2069.jpeg         \n","  inflating: shoes_2076.jpeg         \n","  inflating: shoes_208.jpeg          \n","  inflating: shoes_2081.jpeg         \n","  inflating: shoes_2083.jpeg         \n","  inflating: shoes_2087.jpeg         \n","  inflating: shoes_2089.jpeg         \n","  inflating: shoes_2092.jpeg         \n","  inflating: shoes_2097.jpeg         \n","  inflating: shoes_2105.jpeg         \n","  inflating: shoes_2109.jpeg         \n","  inflating: shoes_2113.jpeg         \n","  inflating: shoes_2114.jpeg         \n","  inflating: shoes_2136.jpeg         \n","  inflating: shoes_215.jpeg          \n","  inflating: shoes_2159.jpeg         \n","  inflating: shoes_2160.jpeg         \n","  inflating: shoes_2168.jpeg         \n","  inflating: shoes_2169.jpeg         \n","  inflating: shoes_2171.jpeg         \n","  inflating: shoes_2183.jpeg         \n","  inflating: shoes_2190.jpeg         \n","  inflating: shoes_2192.jpeg         \n","  inflating: shoes_2194.jpeg         \n","  inflating: shoes_2198.jpeg         \n","  inflating: shoes_2199.jpeg         \n","  inflating: shoes_2203.jpeg         \n","  inflating: shoes_2225.jpeg         \n","  inflating: shoes_2227.jpeg         \n","  inflating: shoes_2229.jpeg         \n","  inflating: shoes_2231.jpeg         \n","  inflating: shoes_2245.jpeg         \n","  inflating: shoes_2247.jpeg         \n","  inflating: shoes_2261.jpeg         \n","  inflating: shoes_2267.jpeg         \n","  inflating: shoes_227.jpeg          \n","  inflating: shoes_2270.jpeg         \n","  inflating: shoes_2271.jpeg         \n","  inflating: shoes_2276.jpeg         \n","  inflating: shoes_2277.jpeg         \n","  inflating: shoes_2282.jpeg         \n","  inflating: shoes_2286.jpeg         \n","  inflating: shoes_23.jpeg           \n","  inflating: shoes_2308.jpeg         \n","  inflating: shoes_2312.jpeg         \n","  inflating: shoes_2313.jpeg         \n","  inflating: shoes_2317.jpeg         \n","  inflating: shoes_2320.jpeg         \n","  inflating: shoes_2323.jpeg         \n","  inflating: shoes_2332.jpeg         \n","  inflating: shoes_2337.jpeg         \n","  inflating: shoes_2340.jpeg         \n","  inflating: shoes_2341.jpeg         \n","  inflating: shoes_2344.jpeg         \n","  inflating: shoes_2346.jpeg         \n","  inflating: shoes_2347.jpeg         \n","  inflating: shoes_2355.jpeg         \n","  inflating: shoes_2357.jpeg         \n","  inflating: shoes_2360.jpeg         \n","  inflating: shoes_2362.jpeg         \n","  inflating: shoes_2368.jpeg         \n","  inflating: shoes_2373.jpeg         \n","  inflating: shoes_2377.jpeg         \n","  inflating: shoes_2382.jpeg         \n","  inflating: shoes_2387.jpeg         \n","  inflating: shoes_2394.jpeg         \n","  inflating: shoes_2399.jpeg         \n","  inflating: shoes_2405.jpeg         \n","  inflating: shoes_2406.jpeg         \n","  inflating: shoes_2407.jpeg         \n","  inflating: shoes_2411.jpeg         \n","  inflating: shoes_242.jpeg          \n","  inflating: shoes_2420.jpeg         \n","  inflating: shoes_2426.jpeg         \n","  inflating: shoes_2435.jpeg         \n","  inflating: shoes_2447.jpeg         \n","  inflating: shoes_2460.jpeg         \n","  inflating: shoes_2462.jpeg         \n","  inflating: shoes_2463.jpeg         \n","  inflating: shoes_247.jpeg          \n","  inflating: shoes_2475.jpeg         \n","  inflating: shoes_2485.jpeg         \n","  inflating: shoes_2486.jpeg         \n","  inflating: shoes_249.jpeg          \n","  inflating: shoes_2494.jpeg         \n","  inflating: shoes_2497.jpeg         \n","  inflating: shoes_2502.jpeg         \n","  inflating: shoes_2504.jpeg         \n","  inflating: shoes_2506.jpeg         \n","  inflating: shoes_251.jpeg          \n","  inflating: shoes_2510.jpeg         \n","  inflating: shoes_2513.jpeg         \n","  inflating: shoes_2517.jpeg         \n","  inflating: shoes_2520.jpeg         \n","  inflating: shoes_2524.jpeg         \n","  inflating: shoes_2525.jpeg         \n","  inflating: shoes_255.jpeg          \n","  inflating: shoes_2551.jpeg         \n","  inflating: shoes_2555.jpeg         \n","  inflating: shoes_2563.jpeg         \n","  inflating: shoes_2570.jpeg         \n","  inflating: shoes_2575.jpeg         \n","  inflating: shoes_2578.jpeg         \n","  inflating: shoes_2582.jpeg         \n","  inflating: shoes_2583.jpeg         \n","  inflating: shoes_2585.jpeg         \n","  inflating: shoes_259.jpeg          \n","  inflating: shoes_2594.jpeg         \n","  inflating: shoes_2598.jpeg         \n","  inflating: shoes_260.jpeg          \n","  inflating: shoes_2600.jpeg         \n","  inflating: shoes_2604.jpeg         \n","  inflating: shoes_2606.jpeg         \n","  inflating: shoes_2610.jpeg         \n","  inflating: shoes_2612.jpeg         \n","  inflating: shoes_2619.jpeg         \n","  inflating: shoes_263.jpeg          \n","  inflating: shoes_2633.jpeg         \n","  inflating: shoes_2638.jpeg         \n","  inflating: shoes_2639.jpeg         \n","  inflating: shoes_2645.jpeg         \n","  inflating: shoes_2650.jpeg         \n","  inflating: shoes_2653.jpeg         \n","  inflating: shoes_2654.jpeg         \n","  inflating: shoes_2656.jpeg         \n","  inflating: shoes_266.jpeg          \n","  inflating: shoes_2663.jpeg         \n","  inflating: shoes_2664.jpeg         \n","  inflating: shoes_2669.jpeg         \n","  inflating: shoes_2677.jpeg         \n","  inflating: shoes_2682.jpeg         \n","  inflating: shoes_2695.jpeg         \n","  inflating: shoes_2697.jpeg         \n","  inflating: shoes_2698.jpeg         \n","  inflating: shoes_27.jpeg           \n","  inflating: shoes_2704.jpeg         \n","  inflating: shoes_2707.jpeg         \n","  inflating: shoes_2708.jpeg         \n","  inflating: shoes_2712.jpeg         \n","  inflating: shoes_2719.jpeg         \n","  inflating: shoes_2721.jpeg         \n","  inflating: shoes_2726.jpeg         \n","  inflating: shoes_273.jpeg          \n","  inflating: shoes_2735.jpeg         \n","  inflating: shoes_2737.jpeg         \n","  inflating: shoes_274.jpeg          \n","  inflating: shoes_2748.jpeg         \n","  inflating: shoes_275.jpeg          \n","  inflating: shoes_2756.jpeg         \n","  inflating: shoes_2763.jpeg         \n","  inflating: shoes_2766.jpeg         \n","  inflating: shoes_2767.jpeg         \n","  inflating: shoes_2776.jpeg         \n","  inflating: shoes_2782.jpeg         \n","  inflating: shoes_2789.jpeg         \n","  inflating: shoes_2793.jpeg         \n","  inflating: shoes_2795.jpeg         \n","  inflating: shoes_2799.jpeg         \n","  inflating: shoes_2800.jpeg         \n","  inflating: shoes_2817.jpeg         \n","  inflating: shoes_2820.jpeg         \n","  inflating: shoes_2826.jpeg         \n","  inflating: shoes_2832.jpeg         \n","  inflating: shoes_2838.jpeg         \n","  inflating: shoes_284.jpeg          \n","  inflating: shoes_2845.jpeg         \n","  inflating: shoes_2846.jpeg         \n","  inflating: shoes_2854.jpeg         \n","  inflating: shoes_2860.jpeg         \n","  inflating: shoes_2869.jpeg         \n","  inflating: shoes_2871.jpeg         \n","  inflating: shoes_2889.jpeg         \n","  inflating: shoes_289.jpeg          \n","  inflating: shoes_2893.jpeg         \n","  inflating: shoes_2894.jpeg         \n","  inflating: shoes_2896.jpeg         \n","  inflating: shoes_2898.jpeg         \n","  inflating: shoes_2906.jpeg         \n","  inflating: shoes_2913.jpeg         \n","  inflating: shoes_2917.jpeg         \n","  inflating: shoes_2923.jpeg         \n","  inflating: shoes_2927.jpeg         \n","  inflating: shoes_2934.jpeg         \n","  inflating: shoes_2936.jpeg         \n","  inflating: shoes_2944.jpeg         \n","  inflating: shoes_2951.jpeg         \n","  inflating: shoes_2953.jpeg         \n","  inflating: shoes_2959.jpeg         \n","  inflating: shoes_296.jpeg          \n","  inflating: shoes_2960.jpeg         \n","  inflating: shoes_2969.jpeg         \n","  inflating: shoes_2970.jpeg         \n","  inflating: shoes_2977.jpeg         \n","  inflating: shoes_2978.jpeg         \n","  inflating: shoes_300.jpeg          \n","  inflating: shoes_3009.jpeg         \n","  inflating: shoes_3011.jpeg         \n","  inflating: shoes_3027.jpeg         \n","  inflating: shoes_3031.jpeg         \n","  inflating: shoes_3032.jpeg         \n","  inflating: shoes_3036.jpeg         \n","  inflating: shoes_3042.jpeg         \n","  inflating: shoes_3052.jpeg         \n","  inflating: shoes_3053.jpeg         \n","  inflating: shoes_3054.jpeg         \n","  inflating: shoes_3059.jpeg         \n","  inflating: shoes_3079.jpeg         \n","  inflating: shoes_3089.jpeg         \n","  inflating: shoes_3090.jpeg         \n","  inflating: shoes_3092.jpeg         \n","  inflating: shoes_3093.jpeg         \n","  inflating: shoes_3098.jpeg         \n","  inflating: shoes_31.jpeg           \n","  inflating: shoes_3101.jpeg         \n","  inflating: shoes_3102.jpeg         \n","  inflating: shoes_3104.jpeg         \n","  inflating: shoes_3106.jpeg         \n","  inflating: shoes_312.jpeg          \n","  inflating: shoes_3125.jpeg         \n","  inflating: shoes_3131.jpeg         \n","  inflating: shoes_3134.jpeg         \n","  inflating: shoes_3139.jpeg         \n","  inflating: shoes_3149.jpeg         \n","  inflating: shoes_315.jpeg          \n","  inflating: shoes_3151.jpeg         \n","  inflating: shoes_3161.jpeg         \n","  inflating: shoes_3166.jpeg         \n","  inflating: shoes_3169.jpeg         \n","  inflating: shoes_3173.jpeg         \n","  inflating: shoes_3181.jpeg         \n","  inflating: shoes_3184.jpeg         \n","  inflating: shoes_3199.jpeg         \n","  inflating: shoes_3200.jpeg         \n","  inflating: shoes_321.jpeg          \n","  inflating: shoes_3210.jpeg         \n","  inflating: shoes_3219.jpeg         \n","  inflating: shoes_3221.jpeg         \n","  inflating: shoes_3222.jpeg         \n","  inflating: shoes_3229.jpeg         \n","  inflating: shoes_323.jpeg          \n","  inflating: shoes_3238.jpeg         \n","  inflating: shoes_3240.jpeg         \n","  inflating: shoes_3245.jpeg         \n","  inflating: shoes_3250.jpeg         \n","  inflating: shoes_3251.jpeg         \n","  inflating: shoes_3263.jpeg         \n","  inflating: shoes_3277.jpeg         \n","  inflating: shoes_3283.jpeg         \n","  inflating: shoes_3288.jpeg         \n","  inflating: shoes_3298.jpeg         \n","  inflating: shoes_3305.jpeg         \n","  inflating: shoes_3306.jpeg         \n","  inflating: shoes_3310.jpeg         \n","  inflating: shoes_3311.jpeg         \n","  inflating: shoes_3317.jpeg         \n","  inflating: shoes_3318.jpeg         \n","  inflating: shoes_332.jpeg          \n","  inflating: shoes_3325.jpeg         \n","  inflating: shoes_3329.jpeg         \n","  inflating: shoes_3331.jpeg         \n","  inflating: shoes_3332.jpeg         \n","  inflating: shoes_3339.jpeg         \n","  inflating: shoes_3342.jpeg         \n","  inflating: shoes_3345.jpeg         \n","  inflating: shoes_337.jpeg          \n","  inflating: shoes_3372.jpeg         \n","  inflating: shoes_3385.jpeg         \n","  inflating: shoes_3392.jpeg         \n","  inflating: shoes_3393.jpeg         \n","  inflating: shoes_3396.jpeg         \n","  inflating: shoes_3406.jpeg         \n","  inflating: shoes_3413.jpeg         \n","  inflating: shoes_3418.jpeg         \n","  inflating: shoes_3420.jpeg         \n","  inflating: shoes_3421.jpeg         \n","  inflating: shoes_3423.jpeg         \n","  inflating: shoes_3432.jpeg         \n","  inflating: shoes_3436.jpeg         \n","  inflating: shoes_3439.jpeg         \n","  inflating: shoes_3441.jpeg         \n","  inflating: shoes_3452.jpeg         \n","  inflating: shoes_3453.jpeg         \n","  inflating: shoes_3458.jpeg         \n","  inflating: shoes_3459.jpeg         \n","  inflating: shoes_3474.jpeg         \n","  inflating: shoes_3476.jpeg         \n","  inflating: shoes_3477.jpeg         \n","  inflating: shoes_3484.jpeg         \n","  inflating: shoes_3488.jpeg         \n","  inflating: shoes_349.jpeg          \n","  inflating: shoes_3490.jpeg         \n","  inflating: shoes_3496.jpeg         \n","  inflating: shoes_3502.jpeg         \n","  inflating: shoes_3506.jpeg         \n","  inflating: shoes_3508.jpeg         \n","  inflating: shoes_351.jpeg          \n","  inflating: shoes_3511.jpeg         \n","  inflating: shoes_3518.jpeg         \n","  inflating: shoes_3520.jpeg         \n","  inflating: shoes_3522.jpeg         \n","  inflating: shoes_3526.jpeg         \n","  inflating: shoes_3527.jpeg         \n","  inflating: shoes_3543.jpeg         \n","  inflating: shoes_3544.jpeg         \n","  inflating: shoes_3547.jpeg         \n","  inflating: shoes_3556.jpeg         \n","  inflating: shoes_3558.jpeg         \n","  inflating: shoes_3559.jpeg         \n","  inflating: shoes_3560.jpeg         \n","  inflating: shoes_3563.jpeg         \n","  inflating: shoes_3564.jpeg         \n","  inflating: shoes_3574.jpeg         \n","  inflating: shoes_3578.jpeg         \n","  inflating: shoes_3579.jpeg         \n","  inflating: shoes_3580.jpeg         \n","  inflating: shoes_3583.jpeg         \n","  inflating: shoes_3584.jpeg         \n","  inflating: shoes_3588.jpeg         \n","  inflating: shoes_3592.jpeg         \n","  inflating: shoes_3595.jpeg         \n","  inflating: shoes_3599.jpeg         \n","  inflating: shoes_36.jpeg           \n","  inflating: shoes_360.jpeg          \n","  inflating: shoes_3608.jpeg         \n","  inflating: shoes_3612.jpeg         \n","  inflating: shoes_3613.jpeg         \n","  inflating: shoes_3615.jpeg         \n","  inflating: shoes_362.jpeg          \n","  inflating: shoes_3620.jpeg         \n","  inflating: shoes_3627.jpeg         \n","  inflating: shoes_3629.jpeg         \n","  inflating: shoes_3636.jpeg         \n","  inflating: shoes_365.jpeg          \n","  inflating: shoes_3651.jpeg         \n","  inflating: shoes_3669.jpeg         \n","  inflating: shoes_3679.jpeg         \n","  inflating: shoes_368.jpeg          \n","  inflating: shoes_3680.jpeg         \n","  inflating: shoes_3682.jpeg         \n","  inflating: shoes_3687.jpeg         \n","  inflating: shoes_3690.jpeg         \n","  inflating: shoes_3695.jpeg         \n","  inflating: shoes_37.jpeg           \n","  inflating: shoes_3701.jpeg         \n","  inflating: shoes_3702.jpeg         \n","  inflating: shoes_3704.jpeg         \n","  inflating: shoes_3714.jpeg         \n","  inflating: shoes_3719.jpeg         \n","  inflating: shoes_3727.jpeg         \n","  inflating: shoes_373.jpeg          \n","  inflating: shoes_3737.jpeg         \n","  inflating: shoes_3740.jpeg         \n","  inflating: shoes_3741.jpeg         \n","  inflating: shoes_3742.jpeg         \n","  inflating: shoes_3745.jpeg         \n","  inflating: shoes_3746.jpeg         \n","  inflating: shoes_3749.jpeg         \n","  inflating: shoes_3750.jpeg         \n","  inflating: shoes_3754.jpeg         \n","  inflating: shoes_3757.jpeg         \n","  inflating: shoes_3771.jpeg         \n","  inflating: shoes_3781.jpeg         \n","  inflating: shoes_3786.jpeg         \n","  inflating: shoes_379.jpeg          \n","  inflating: shoes_3793.jpeg         \n","  inflating: shoes_3804.jpeg         \n","  inflating: shoes_3808.jpeg         \n","  inflating: shoes_3817.jpeg         \n","  inflating: shoes_3819.jpeg         \n","  inflating: shoes_3820.jpeg         \n","  inflating: shoes_3836.jpeg         \n","  inflating: shoes_3837.jpeg         \n","  inflating: shoes_3841.jpeg         \n","  inflating: shoes_3842.jpeg         \n","  inflating: shoes_3843.jpeg         \n","  inflating: shoes_3844.jpeg         \n","  inflating: shoes_3851.jpeg         \n","  inflating: shoes_3857.jpeg         \n","  inflating: shoes_3868.jpeg         \n","  inflating: shoes_3873.jpeg         \n","  inflating: shoes_3886.jpeg         \n","  inflating: shoes_3888.jpeg         \n","  inflating: shoes_389.jpeg          \n","  inflating: shoes_3890.jpeg         \n","  inflating: shoes_3894.jpeg         \n","  inflating: shoes_3899.jpeg         \n","  inflating: shoes_3906.jpeg         \n","  inflating: shoes_3907.jpeg         \n","  inflating: shoes_3911.jpeg         \n","  inflating: shoes_3913.jpeg         \n","  inflating: shoes_3918.jpeg         \n","  inflating: shoes_392.jpeg          \n","  inflating: shoes_3925.jpeg         \n","  inflating: shoes_3929.jpeg         \n","  inflating: shoes_393.jpeg          \n","  inflating: shoes_3933.jpeg         \n","  inflating: shoes_3936.jpeg         \n","  inflating: shoes_3960.jpeg         \n","  inflating: shoes_3964.jpeg         \n","  inflating: shoes_3968.jpeg         \n","  inflating: shoes_397.jpeg          \n","  inflating: shoes_3971.jpeg         \n","  inflating: shoes_3976.jpeg         \n","  inflating: shoes_3978.jpeg         \n","  inflating: shoes_3979.jpeg         \n","  inflating: shoes_3988.jpeg         \n","  inflating: shoes_40.jpeg           \n","  inflating: shoes_4009.jpeg         \n","  inflating: shoes_4012.jpeg         \n","  inflating: shoes_4013.jpeg         \n","  inflating: shoes_4014.jpeg         \n","  inflating: shoes_4027.jpeg         \n","  inflating: shoes_4030.jpeg         \n","  inflating: shoes_4035.jpeg         \n","  inflating: shoes_405.jpeg          \n","  inflating: shoes_4054.jpeg         \n","  inflating: shoes_4057.jpeg         \n","  inflating: shoes_4060.jpeg         \n","  inflating: shoes_4064.jpeg         \n","  inflating: shoes_4068.jpeg         \n","  inflating: shoes_407.jpeg          \n","  inflating: shoes_4074.jpeg         \n","  inflating: shoes_4075.jpeg         \n","  inflating: shoes_4084.jpeg         \n","  inflating: shoes_409.jpeg          \n","  inflating: shoes_4098.jpeg         \n","  inflating: shoes_4100.jpeg         \n","  inflating: shoes_4104.jpeg         \n","  inflating: shoes_4108.jpeg         \n","  inflating: shoes_4112.jpeg         \n","  inflating: shoes_4121.jpeg         \n","  inflating: shoes_4122.jpeg         \n","  inflating: shoes_4126.jpeg         \n","  inflating: shoes_4130.jpeg         \n","  inflating: shoes_4136.jpeg         \n","  inflating: shoes_4141.jpeg         \n","  inflating: shoes_4142.jpeg         \n","  inflating: shoes_4155.jpeg         \n","  inflating: shoes_4160.jpeg         \n","  inflating: shoes_4162.jpeg         \n","  inflating: shoes_4165.jpeg         \n","  inflating: shoes_4168.jpeg         \n","  inflating: shoes_4175.jpeg         \n","  inflating: shoes_4176.jpeg         \n","  inflating: shoes_4186.jpeg         \n","  inflating: shoes_419.jpeg          \n","  inflating: shoes_4195.jpeg         \n","  inflating: shoes_4198.jpeg         \n","  inflating: shoes_42.jpeg           \n","  inflating: shoes_4202.jpeg         \n","  inflating: shoes_4206.jpeg         \n","  inflating: shoes_4215.jpeg         \n","  inflating: shoes_422.jpeg          \n","  inflating: shoes_4223.jpeg         \n","  inflating: shoes_4224.jpeg         \n","  inflating: shoes_423.jpeg          \n","  inflating: shoes_4234.jpeg         \n","  inflating: shoes_4240.jpeg         \n","  inflating: shoes_4243.jpeg         \n","  inflating: shoes_4249.jpeg         \n","  inflating: shoes_4251.jpeg         \n","  inflating: shoes_4252.jpeg         \n","  inflating: shoes_426.jpeg          \n","  inflating: shoes_4265.jpeg         \n","  inflating: shoes_4272.jpeg         \n","  inflating: shoes_4281.jpeg         \n","  inflating: shoes_4286.jpeg         \n","  inflating: shoes_4291.jpeg         \n","  inflating: shoes_4296.jpeg         \n","  inflating: shoes_4298.jpeg         \n","  inflating: shoes_43.jpeg           \n","  inflating: shoes_4303.jpeg         \n","  inflating: shoes_4305.jpeg         \n","  inflating: shoes_4312.jpeg         \n","  inflating: shoes_4315.jpeg         \n","  inflating: shoes_4317.jpeg         \n","  inflating: shoes_432.jpeg          \n","  inflating: shoes_4321.jpeg         \n","  inflating: shoes_4323.jpeg         \n","  inflating: shoes_4328.jpeg         \n","  inflating: shoes_4331.jpeg         \n","  inflating: shoes_4332.jpeg         \n","  inflating: shoes_4334.jpeg         \n","  inflating: shoes_4344.jpeg         \n","  inflating: shoes_4371.jpeg         \n","  inflating: shoes_4374.jpeg         \n","  inflating: shoes_4380.jpeg         \n","  inflating: shoes_4383.jpeg         \n","  inflating: shoes_4384.jpeg         \n","  inflating: shoes_4399.jpeg         \n","  inflating: shoes_4408.jpeg         \n","  inflating: shoes_4412.jpeg         \n","  inflating: shoes_4415.jpeg         \n","  inflating: shoes_4427.jpeg         \n","  inflating: shoes_443.jpeg          \n","  inflating: shoes_4436.jpeg         \n","  inflating: shoes_4437.jpeg         \n","  inflating: shoes_4439.jpeg         \n","  inflating: shoes_4441.jpeg         \n","  inflating: shoes_4447.jpeg         \n","  inflating: shoes_4452.jpeg         \n","  inflating: shoes_4456.jpeg         \n","  inflating: shoes_446.jpeg          \n","  inflating: shoes_4468.jpeg         \n","  inflating: shoes_4470.jpeg         \n","  inflating: shoes_4471.jpeg         \n","  inflating: shoes_4475.jpeg         \n","  inflating: shoes_4476.jpeg         \n","  inflating: shoes_4483.jpeg         \n","  inflating: shoes_4489.jpeg         \n","  inflating: shoes_4490.jpeg         \n","  inflating: shoes_4492.jpeg         \n","  inflating: shoes_4496.jpeg         \n","  inflating: shoes_4499.jpeg         \n","  inflating: shoes_45.jpeg           \n","  inflating: shoes_4507.jpeg         \n","  inflating: shoes_4513.jpeg         \n","  inflating: shoes_4517.jpeg         \n","  inflating: shoes_4518.jpeg         \n","  inflating: shoes_4522.jpeg         \n","  inflating: shoes_4525.jpeg         \n","  inflating: shoes_4527.jpeg         \n","  inflating: shoes_4531.jpeg         \n","  inflating: shoes_4533.jpeg         \n","  inflating: shoes_4535.jpeg         \n","  inflating: shoes_4546.jpeg         \n","  inflating: shoes_4547.jpeg         \n","  inflating: shoes_4548.jpeg         \n","  inflating: shoes_4552.jpeg         \n","  inflating: shoes_4556.jpeg         \n","  inflating: shoes_4559.jpeg         \n","  inflating: shoes_4561.jpeg         \n","  inflating: shoes_4565.jpeg         \n","  inflating: shoes_4568.jpeg         \n","  inflating: shoes_4575.jpeg         \n","  inflating: shoes_4579.jpeg         \n","  inflating: shoes_4581.jpeg         \n","  inflating: shoes_4587.jpeg         \n","  inflating: shoes_4592.jpeg         \n","  inflating: shoes_4602.jpeg         \n","  inflating: shoes_4606.jpeg         \n","  inflating: shoes_4611.jpeg         \n","  inflating: shoes_4612.jpeg         \n","  inflating: shoes_4620.jpeg         \n","  inflating: shoes_4622.jpeg         \n","  inflating: shoes_4623.jpeg         \n","  inflating: shoes_4624.jpeg         \n","  inflating: shoes_4627.jpeg         \n","  inflating: shoes_4628.jpeg         \n","  inflating: shoes_4641.jpeg         \n","  inflating: shoes_4645.jpeg         \n","  inflating: shoes_4648.jpeg         \n","  inflating: shoes_465.jpeg          \n","  inflating: shoes_4651.jpeg         \n","  inflating: shoes_4658.jpeg         \n","  inflating: shoes_4663.jpeg         \n","  inflating: shoes_4676.jpeg         \n","  inflating: shoes_47.jpeg           \n","  inflating: shoes_470.jpeg          \n","  inflating: shoes_4704.jpeg         \n","  inflating: shoes_4715.jpeg         \n","  inflating: shoes_4725.jpeg         \n","  inflating: shoes_4726.jpeg         \n","  inflating: shoes_4735.jpeg         \n","  inflating: shoes_4739.jpeg         \n","  inflating: shoes_4742.jpeg         \n","  inflating: shoes_4743.jpeg         \n","  inflating: shoes_4744.jpeg         \n","  inflating: shoes_4745.jpeg         \n","  inflating: shoes_4747.jpeg         \n","  inflating: shoes_4748.jpeg         \n","  inflating: shoes_4751.jpeg         \n","  inflating: shoes_4758.jpeg         \n","  inflating: shoes_4760.jpeg         \n","  inflating: shoes_4762.jpeg         \n","  inflating: shoes_4763.jpeg         \n","  inflating: shoes_477.jpeg          \n","  inflating: shoes_478.jpeg          \n","  inflating: shoes_4781.jpeg         \n","  inflating: shoes_4785.jpeg         \n","  inflating: shoes_4787.jpeg         \n","  inflating: shoes_4795.jpeg         \n","  inflating: shoes_4797.jpeg         \n","  inflating: shoes_48.jpeg           \n","  inflating: shoes_4800.jpeg         \n","  inflating: shoes_4803.jpeg         \n","  inflating: shoes_481.jpeg          \n","  inflating: shoes_4810.jpeg         \n","  inflating: shoes_4816.jpeg         \n","  inflating: shoes_482.jpeg          \n","  inflating: shoes_4821.jpeg         \n","  inflating: shoes_4822.jpeg         \n","  inflating: shoes_4829.jpeg         \n","  inflating: shoes_483.jpeg          \n","  inflating: shoes_4832.jpeg         \n","  inflating: shoes_4840.jpeg         \n","  inflating: shoes_4843.jpeg         \n","  inflating: shoes_4858.jpeg         \n","  inflating: shoes_4862.jpeg         \n","  inflating: shoes_4872.jpeg         \n","  inflating: shoes_488.jpeg          \n","  inflating: shoes_489.jpeg          \n","  inflating: shoes_4899.jpeg         \n","  inflating: shoes_490.jpeg          \n","  inflating: shoes_4901.jpeg         \n","  inflating: shoes_4913.jpeg         \n","  inflating: shoes_4919.jpeg         \n","  inflating: shoes_493.jpeg          \n","  inflating: shoes_4951.jpeg         \n","  inflating: shoes_4953.jpeg         \n","  inflating: shoes_4956.jpeg         \n","  inflating: shoes_4969.jpeg         \n","  inflating: shoes_497.jpeg          \n","  inflating: shoes_4974.jpeg         \n","  inflating: shoes_4976.jpeg         \n","  inflating: shoes_4983.jpeg         \n","  inflating: shoes_4988.jpeg         \n","  inflating: shoes_4997.jpeg         \n","  inflating: shoes_5.jpeg            \n","  inflating: shoes_500.jpeg          \n","  inflating: shoes_5006.jpeg         \n","  inflating: shoes_5012.jpeg         \n","  inflating: shoes_5014.jpeg         \n","  inflating: shoes_5015.jpeg         \n","  inflating: shoes_5016.jpeg         \n","  inflating: shoes_5031.jpeg         \n","  inflating: shoes_5033.jpeg         \n","  inflating: shoes_5037.jpeg         \n","  inflating: shoes_5042.jpeg         \n","  inflating: shoes_5049.jpeg         \n","  inflating: shoes_5055.jpeg         \n","  inflating: shoes_5066.jpeg         \n","  inflating: shoes_5067.jpeg         \n","  inflating: shoes_507.jpeg          \n","  inflating: shoes_5075.jpeg         \n","  inflating: shoes_5100.jpeg         \n","  inflating: shoes_5109.jpeg         \n","  inflating: shoes_512.jpeg          \n","  inflating: shoes_5121.jpeg         \n","  inflating: shoes_5125.jpeg         \n","  inflating: shoes_5126.jpeg         \n","  inflating: shoes_5134.jpeg         \n","  inflating: shoes_514.jpeg          \n","  inflating: shoes_5143.jpeg         \n","  inflating: shoes_5146.jpeg         \n","  inflating: shoes_5166.jpeg         \n","  inflating: shoes_5171.jpeg         \n","  inflating: shoes_5172.jpeg         \n","  inflating: shoes_5179.jpeg         \n","  inflating: shoes_518.jpeg          \n","  inflating: shoes_5181.jpeg         \n","  inflating: shoes_5189.jpeg         \n","  inflating: shoes_5192.jpeg         \n","  inflating: shoes_5193.jpeg         \n","  inflating: shoes_5196.jpeg         \n","  inflating: shoes_52.jpeg           \n","  inflating: shoes_5203.jpeg         \n","  inflating: shoes_5208.jpeg         \n","  inflating: shoes_5214.jpeg         \n","  inflating: shoes_522.jpeg          \n","  inflating: shoes_5221.jpeg         \n","  inflating: shoes_5225.jpeg         \n","  inflating: shoes_5236.jpeg         \n","  inflating: shoes_5241.jpeg         \n","  inflating: shoes_5242.jpeg         \n","  inflating: shoes_5243.jpeg         \n","  inflating: shoes_5244.jpeg         \n","  inflating: shoes_5245.jpeg         \n","  inflating: shoes_525.jpeg          \n","  inflating: shoes_5261.jpeg         \n","  inflating: shoes_5267.jpeg         \n","  inflating: shoes_5272.jpeg         \n","  inflating: shoes_5277.jpeg         \n","  inflating: shoes_5291.jpeg         \n","  inflating: shoes_5303.jpeg         \n","  inflating: shoes_5309.jpeg         \n","  inflating: shoes_5314.jpeg         \n","  inflating: shoes_5319.jpeg         \n","  inflating: shoes_5323.jpeg         \n","  inflating: shoes_5327.jpeg         \n","  inflating: shoes_5328.jpeg         \n","  inflating: shoes_5335.jpeg         \n","  inflating: shoes_5337.jpeg         \n","  inflating: shoes_534.jpeg          \n","  inflating: shoes_5345.jpeg         \n","  inflating: shoes_5355.jpeg         \n","  inflating: shoes_5356.jpeg         \n","  inflating: shoes_5358.jpeg         \n","  inflating: shoes_5360.jpeg         \n","  inflating: shoes_5363.jpeg         \n","  inflating: shoes_5366.jpeg         \n","  inflating: shoes_5367.jpeg         \n","  inflating: shoes_5374.jpeg         \n","  inflating: shoes_5377.jpeg         \n","  inflating: shoes_5383.jpeg         \n","  inflating: shoes_5385.jpeg         \n","  inflating: shoes_5387.jpeg         \n","  inflating: shoes_5403.jpeg         \n","  inflating: shoes_5404.jpeg         \n","  inflating: shoes_5405.jpeg         \n","  inflating: shoes_542.jpeg          \n","  inflating: shoes_5426.jpeg         \n","  inflating: shoes_5428.jpeg         \n","  inflating: shoes_5432.jpeg         \n","  inflating: shoes_5441.jpeg         \n","  inflating: shoes_5452.jpeg         \n","  inflating: shoes_546.jpeg          \n","  inflating: shoes_5461.jpeg         \n","  inflating: shoes_5462.jpeg         \n","  inflating: shoes_5463.jpeg         \n","  inflating: shoes_5468.jpeg         \n","  inflating: shoes_547.jpeg          \n","  inflating: shoes_5472.jpeg         \n","  inflating: shoes_5474.jpeg         \n","  inflating: shoes_5479.jpeg         \n","  inflating: shoes_5488.jpeg         \n","  inflating: shoes_5491.jpeg         \n","  inflating: shoes_5495.jpeg         \n","  inflating: shoes_5497.jpeg         \n","  inflating: shoes_5506.jpeg         \n","  inflating: shoes_5513.jpeg         \n","  inflating: shoes_5519.jpeg         \n","  inflating: shoes_5520.jpeg         \n","  inflating: shoes_5532.jpeg         \n","  inflating: shoes_5534.jpeg         \n","  inflating: shoes_5537.jpeg         \n","  inflating: shoes_5541.jpeg         \n","  inflating: shoes_5545.jpeg         \n","  inflating: shoes_5566.jpeg         \n","  inflating: shoes_5568.jpeg         \n","  inflating: shoes_5570.jpeg         \n","  inflating: shoes_5575.jpeg         \n","  inflating: shoes_5578.jpeg         \n","  inflating: shoes_5580.jpeg         \n","  inflating: shoes_5582.jpeg         \n","  inflating: shoes_5583.jpeg         \n","  inflating: shoes_5588.jpeg         \n","  inflating: shoes_559.jpeg          \n","  inflating: shoes_5590.jpeg         \n","  inflating: shoes_5591.jpeg         \n","  inflating: shoes_5596.jpeg         \n","  inflating: shoes_561.jpeg          \n","  inflating: shoes_5620.jpeg         \n","  inflating: shoes_5622.jpeg         \n","  inflating: shoes_5624.jpeg         \n","  inflating: shoes_5632.jpeg         \n","  inflating: shoes_5633.jpeg         \n","  inflating: shoes_5634.jpeg         \n","  inflating: shoes_5636.jpeg         \n","  inflating: shoes_5649.jpeg         \n","  inflating: shoes_5651.jpeg         \n","  inflating: shoes_5657.jpeg         \n","  inflating: shoes_5662.jpeg         \n","  inflating: shoes_5666.jpeg         \n","  inflating: shoes_5668.jpeg         \n","  inflating: shoes_5669.jpeg         \n","  inflating: shoes_5670.jpeg         \n","  inflating: shoes_5672.jpeg         \n","  inflating: shoes_5690.jpeg         \n","  inflating: shoes_5696.jpeg         \n","  inflating: shoes_5697.jpeg         \n","  inflating: shoes_5698.jpeg         \n","  inflating: shoes_5700.jpeg         \n","  inflating: shoes_5704.jpeg         \n","  inflating: shoes_5716.jpeg         \n","  inflating: shoes_5717.jpeg         \n","  inflating: shoes_5720.jpeg         \n","  inflating: shoes_5732.jpeg         \n","  inflating: shoes_5736.jpeg         \n","  inflating: shoes_5742.jpeg         \n","  inflating: shoes_5751.jpeg         \n","  inflating: shoes_5754.jpeg         \n","  inflating: shoes_5760.jpeg         \n","  inflating: shoes_5763.jpeg         \n","  inflating: shoes_5764.jpeg         \n","  inflating: shoes_5769.jpeg         \n","  inflating: shoes_5772.jpeg         \n","  inflating: shoes_5779.jpeg         \n","  inflating: shoes_5783.jpeg         \n","  inflating: shoes_5784.jpeg         \n","  inflating: shoes_5787.jpeg         \n","  inflating: shoes_5788.jpeg         \n","  inflating: shoes_579.jpeg          \n","  inflating: shoes_580.jpeg          \n","  inflating: shoes_5801.jpeg         \n","  inflating: shoes_5802.jpeg         \n","  inflating: shoes_5809.jpeg         \n","  inflating: shoes_5812.jpeg         \n","  inflating: shoes_5813.jpeg         \n","  inflating: shoes_5815.jpeg         \n","  inflating: shoes_5816.jpeg         \n","  inflating: shoes_5817.jpeg         \n","  inflating: shoes_582.jpeg          \n","  inflating: shoes_5820.jpeg         \n","  inflating: shoes_5823.jpeg         \n","  inflating: shoes_5825.jpeg         \n","  inflating: shoes_5830.jpeg         \n","  inflating: shoes_5832.jpeg         \n","  inflating: shoes_5840.jpeg         \n","  inflating: shoes_5845.jpeg         \n","  inflating: shoes_5848.jpeg         \n","  inflating: shoes_5851.jpeg         \n","  inflating: shoes_5854.jpeg         \n","  inflating: shoes_5863.jpeg         \n","  inflating: shoes_5865.jpeg         \n","  inflating: shoes_5867.jpeg         \n","  inflating: shoes_5868.jpeg         \n","  inflating: shoes_5869.jpeg         \n","  inflating: shoes_5876.jpeg         \n","  inflating: shoes_5881.jpeg         \n","  inflating: shoes_5884.jpeg         \n","  inflating: shoes_589.jpeg          \n","  inflating: shoes_5890.jpeg         \n","  inflating: shoes_5898.jpeg         \n","  inflating: shoes_59.jpeg           \n","  inflating: shoes_5904.jpeg         \n","  inflating: shoes_5911.jpeg         \n","  inflating: shoes_5913.jpeg         \n","  inflating: shoes_5920.jpeg         \n","  inflating: shoes_5922.jpeg         \n","  inflating: shoes_5929.jpeg         \n","  inflating: shoes_5936.jpeg         \n","  inflating: shoes_5947.jpeg         \n","  inflating: shoes_5948.jpeg         \n","  inflating: shoes_5949.jpeg         \n","  inflating: shoes_5956.jpeg         \n","  inflating: shoes_5962.jpeg         \n","  inflating: shoes_5969.jpeg         \n","  inflating: shoes_5973.jpeg         \n","  inflating: shoes_598.jpeg          \n","  inflating: shoes_5981.jpeg         \n","  inflating: shoes_5987.jpeg         \n","  inflating: shoes_5993.jpeg         \n","  inflating: shoes_5996.jpeg         \n","  inflating: shoes_5998.jpeg         \n","  inflating: shoes_6001.jpeg         \n","  inflating: shoes_6005.jpeg         \n","  inflating: shoes_6006.jpeg         \n","  inflating: shoes_6009.jpeg         \n","  inflating: shoes_601.jpeg          \n","  inflating: shoes_6025.jpeg         \n","  inflating: shoes_6028.jpeg         \n","  inflating: shoes_6029.jpeg         \n","  inflating: shoes_6031.jpeg         \n","  inflating: shoes_6033.jpeg         \n","  inflating: shoes_6039.jpeg         \n","  inflating: shoes_6041.jpeg         \n","  inflating: shoes_6043.jpeg         \n","  inflating: shoes_6054.jpeg         \n","  inflating: shoes_6056.jpeg         \n","  inflating: shoes_6062.jpeg         \n","  inflating: shoes_6065.jpeg         \n","  inflating: shoes_6067.jpeg         \n","  inflating: shoes_6069.jpeg         \n","  inflating: shoes_607.jpeg          \n","  inflating: shoes_6089.jpeg         \n","  inflating: shoes_609.jpeg          \n","  inflating: shoes_6092.jpeg         \n","  inflating: shoes_61.jpeg           \n","  inflating: shoes_6103.jpeg         \n","  inflating: shoes_6108.jpeg         \n","  inflating: shoes_6111.jpeg         \n","  inflating: shoes_6118.jpeg         \n","  inflating: shoes_612.jpeg          \n","  inflating: shoes_6124.jpeg         \n","  inflating: shoes_6130.jpeg         \n","  inflating: shoes_6142.jpeg         \n","  inflating: shoes_6148.jpeg         \n","  inflating: shoes_615.jpeg          \n","  inflating: shoes_6154.jpeg         \n","  inflating: shoes_6155.jpeg         \n","  inflating: shoes_6160.jpeg         \n","  inflating: shoes_6163.jpeg         \n","  inflating: shoes_6168.jpeg         \n","  inflating: shoes_6169.jpeg         \n","  inflating: shoes_6175.jpeg         \n","  inflating: shoes_6182.jpeg         \n","  inflating: shoes_6195.jpeg         \n","  inflating: shoes_6204.jpeg         \n","  inflating: shoes_6209.jpeg         \n","  inflating: shoes_621.jpeg          \n","  inflating: shoes_6221.jpeg         \n","  inflating: shoes_6225.jpeg         \n","  inflating: shoes_6229.jpeg         \n","  inflating: shoes_6230.jpeg         \n","  inflating: shoes_6231.jpeg         \n","  inflating: shoes_6239.jpeg         \n","  inflating: shoes_6245.jpeg         \n","  inflating: shoes_6246.jpeg         \n","  inflating: shoes_6255.jpeg         \n","  inflating: shoes_6265.jpeg         \n","  inflating: shoes_6271.jpeg         \n","  inflating: shoes_6276.jpeg         \n","  inflating: shoes_6278.jpeg         \n","  inflating: shoes_6284.jpeg         \n","  inflating: shoes_6286.jpeg         \n","  inflating: shoes_6290.jpeg         \n","  inflating: shoes_6299.jpeg         \n","  inflating: shoes_63.jpeg           \n","  inflating: shoes_6302.jpeg         \n","  inflating: shoes_6304.jpeg         \n","  inflating: shoes_6305.jpeg         \n","  inflating: shoes_6307.jpeg         \n","  inflating: shoes_6309.jpeg         \n","  inflating: shoes_6313.jpeg         \n","  inflating: shoes_6315.jpeg         \n","  inflating: shoes_632.jpeg          \n","  inflating: shoes_6323.jpeg         \n","  inflating: shoes_6326.jpeg         \n","  inflating: shoes_6327.jpeg         \n","  inflating: shoes_6329.jpeg         \n","  inflating: shoes_6355.jpeg         \n","  inflating: shoes_6365.jpeg         \n","  inflating: shoes_6368.jpeg         \n","  inflating: shoes_6379.jpeg         \n","  inflating: shoes_6380.jpeg         \n","  inflating: shoes_6393.jpeg         \n","  inflating: shoes_6396.jpeg         \n","  inflating: shoes_6404.jpeg         \n","  inflating: shoes_6405.jpeg         \n","  inflating: shoes_6406.jpeg         \n","  inflating: shoes_6411.jpeg         \n","  inflating: shoes_6414.jpeg         \n","  inflating: shoes_6418.jpeg         \n","  inflating: shoes_6419.jpeg         \n","  inflating: shoes_6421.jpeg         \n","  inflating: shoes_643.jpeg          \n","  inflating: shoes_6434.jpeg         \n","  inflating: shoes_6439.jpeg         \n","  inflating: shoes_6440.jpeg         \n","  inflating: shoes_6445.jpeg         \n","  inflating: shoes_6447.jpeg         \n","  inflating: shoes_6448.jpeg         \n","  inflating: shoes_6449.jpeg         \n","  inflating: shoes_6450.jpeg         \n","  inflating: shoes_6465.jpeg         \n","  inflating: shoes_6478.jpeg         \n","  inflating: shoes_6483.jpeg         \n","  inflating: shoes_6484.jpeg         \n","  inflating: shoes_6489.jpeg         \n","  inflating: shoes_65.jpeg           \n","  inflating: shoes_6502.jpeg         \n","  inflating: shoes_6507.jpeg         \n","  inflating: shoes_6508.jpeg         \n","  inflating: shoes_6513.jpeg         \n","  inflating: shoes_6516.jpeg         \n","  inflating: shoes_6522.jpeg         \n","  inflating: shoes_6524.jpeg         \n","  inflating: shoes_6528.jpeg         \n","  inflating: shoes_653.jpeg          \n","  inflating: shoes_6531.jpeg         \n","  inflating: shoes_6534.jpeg         \n","  inflating: shoes_6536.jpeg         \n","  inflating: shoes_6539.jpeg         \n","  inflating: shoes_6542.jpeg         \n","  inflating: shoes_6547.jpeg         \n","  inflating: shoes_6549.jpeg         \n","  inflating: shoes_6550.jpeg         \n","  inflating: shoes_6552.jpeg         \n","  inflating: shoes_6572.jpeg         \n","  inflating: shoes_6576.jpeg         \n","  inflating: shoes_6577.jpeg         \n","  inflating: shoes_658.jpeg          \n","  inflating: shoes_6583.jpeg         \n","  inflating: shoes_6584.jpeg         \n","  inflating: shoes_6593.jpeg         \n","  inflating: shoes_6599.jpeg         \n","  inflating: shoes_660.jpeg          \n","  inflating: shoes_6600.jpeg         \n","  inflating: shoes_6601.jpeg         \n","  inflating: shoes_6605.jpeg         \n","  inflating: shoes_6611.jpeg         \n","  inflating: shoes_6623.jpeg         \n","  inflating: shoes_6624.jpeg         \n","  inflating: shoes_663.jpeg          \n","  inflating: shoes_6632.jpeg         \n","  inflating: shoes_6633.jpeg         \n","  inflating: shoes_6635.jpeg         \n","  inflating: shoes_6637.jpeg         \n","  inflating: shoes_6644.jpeg         \n","  inflating: shoes_6649.jpeg         \n","  inflating: shoes_6663.jpeg         \n","  inflating: shoes_6672.jpeg         \n","  inflating: shoes_6674.jpeg         \n","  inflating: shoes_6676.jpeg         \n","  inflating: shoes_6680.jpeg         \n","  inflating: shoes_6686.jpeg         \n","  inflating: shoes_6688.jpeg         \n","  inflating: shoes_6689.jpeg         \n","  inflating: shoes_6694.jpeg         \n","  inflating: shoes_670.jpeg          \n","  inflating: shoes_6704.jpeg         \n","  inflating: shoes_6711.jpeg         \n","  inflating: shoes_6712.jpeg         \n","  inflating: shoes_6716.jpeg         \n","  inflating: shoes_6718.jpeg         \n","  inflating: shoes_6744.jpeg         \n","  inflating: shoes_6747.jpeg         \n","  inflating: shoes_675.jpeg          \n","  inflating: shoes_6750.jpeg         \n","  inflating: shoes_6752.jpeg         \n","  inflating: shoes_6753.jpeg         \n","  inflating: shoes_6755.jpeg         \n","  inflating: shoes_6757.jpeg         \n","  inflating: shoes_6764.jpeg         \n","  inflating: shoes_6765.jpeg         \n","  inflating: shoes_6766.jpeg         \n","  inflating: shoes_6768.jpeg         \n","  inflating: shoes_6778.jpeg         \n","  inflating: shoes_6780.jpeg         \n","  inflating: shoes_6781.jpeg         \n","  inflating: shoes_6789.jpeg         \n","  inflating: shoes_679.jpeg          \n","  inflating: shoes_6794.jpeg         \n","  inflating: shoes_6796.jpeg         \n","  inflating: shoes_6797.jpeg         \n","  inflating: shoes_6799.jpeg         \n","  inflating: shoes_6804.jpeg         \n","  inflating: shoes_6806.jpeg         \n","  inflating: shoes_6811.jpeg         \n","  inflating: shoes_6812.jpeg         \n","  inflating: shoes_6814.jpeg         \n","  inflating: shoes_6817.jpeg         \n","  inflating: shoes_6819.jpeg         \n","  inflating: shoes_6821.jpeg         \n","  inflating: shoes_6832.jpeg         \n","  inflating: shoes_6839.jpeg         \n","  inflating: shoes_6852.jpeg         \n","  inflating: shoes_6855.jpeg         \n","  inflating: shoes_6858.jpeg         \n","  inflating: shoes_6862.jpeg         \n","  inflating: shoes_6865.jpeg         \n","  inflating: shoes_6870.jpeg         \n","  inflating: shoes_6880.jpeg         \n","  inflating: shoes_6891.jpeg         \n","  inflating: shoes_6895.jpeg         \n","  inflating: shoes_6897.jpeg         \n","  inflating: shoes_6898.jpeg         \n","  inflating: shoes_690.jpeg          \n","  inflating: shoes_6901.jpeg         \n","  inflating: shoes_6925.jpeg         \n","  inflating: shoes_6935.jpeg         \n","  inflating: shoes_6937.jpeg         \n","  inflating: shoes_694.jpeg          \n","  inflating: shoes_695.jpeg          \n","  inflating: shoes_6960.jpeg         \n","  inflating: shoes_6961.jpeg         \n","  inflating: shoes_6976.jpeg         \n","  inflating: shoes_6978.jpeg         \n","  inflating: shoes_698.jpeg          \n","  inflating: shoes_6981.jpeg         \n","  inflating: shoes_6989.jpeg         \n","  inflating: shoes_6992.jpeg         \n","  inflating: shoes_6995.jpeg         \n","  inflating: shoes_6996.jpeg         \n","  inflating: shoes_70.jpeg           \n","  inflating: shoes_7000.jpeg         \n","  inflating: shoes_7012.jpeg         \n","  inflating: shoes_7015.jpeg         \n","  inflating: shoes_7021.jpeg         \n","  inflating: shoes_7022.jpeg         \n","  inflating: shoes_7024.jpeg         \n","  inflating: shoes_7033.jpeg         \n","  inflating: shoes_7035.jpeg         \n","  inflating: shoes_7041.jpeg         \n","  inflating: shoes_7043.jpeg         \n","  inflating: shoes_7045.jpeg         \n","  inflating: shoes_7050.jpeg         \n","  inflating: shoes_7053.jpeg         \n","  inflating: shoes_7056.jpeg         \n","  inflating: shoes_7057.jpeg         \n","  inflating: shoes_7063.jpeg         \n","  inflating: shoes_7075.jpeg         \n","  inflating: shoes_708.jpeg          \n","  inflating: shoes_7080.jpeg         \n","  inflating: shoes_7090.jpeg         \n","  inflating: shoes_7096.jpeg         \n","  inflating: shoes_7102.jpeg         \n","  inflating: shoes_7107.jpeg         \n","  inflating: shoes_7110.jpeg         \n","  inflating: shoes_7112.jpeg         \n","  inflating: shoes_7115.jpeg         \n","  inflating: shoes_7116.jpeg         \n","  inflating: shoes_7121.jpeg         \n","  inflating: shoes_7128.jpeg         \n","  inflating: shoes_7134.jpeg         \n","  inflating: shoes_7142.jpeg         \n","  inflating: shoes_7147.jpeg         \n","  inflating: shoes_7152.jpeg         \n","  inflating: shoes_7155.jpeg         \n","  inflating: shoes_7159.jpeg         \n","  inflating: shoes_7173.jpeg         \n","  inflating: shoes_7180.jpeg         \n","  inflating: shoes_7188.jpeg         \n","  inflating: shoes_7189.jpeg         \n","  inflating: shoes_719.jpeg          \n","  inflating: shoes_7190.jpeg         \n","  inflating: shoes_7192.jpeg         \n","  inflating: shoes_7198.jpeg         \n","  inflating: shoes_721.jpeg          \n","  inflating: shoes_7213.jpeg         \n","  inflating: shoes_7220.jpeg         \n","  inflating: shoes_7225.jpeg         \n","  inflating: shoes_7226.jpeg         \n","  inflating: shoes_7230.jpeg         \n","  inflating: shoes_7239.jpeg         \n","  inflating: shoes_7240.jpeg         \n","  inflating: shoes_7244.jpeg         \n","  inflating: shoes_7245.jpeg         \n","  inflating: shoes_7247.jpeg         \n","  inflating: shoes_725.jpeg          \n","  inflating: shoes_7250.jpeg         \n","  inflating: shoes_7254.jpeg         \n","  inflating: shoes_7255.jpeg         \n","  inflating: shoes_7256.jpeg         \n","  inflating: shoes_7259.jpeg         \n","  inflating: shoes_7260.jpeg         \n","  inflating: shoes_7264.jpeg         \n","  inflating: shoes_7272.jpeg         \n","  inflating: shoes_7278.jpeg         \n","  inflating: shoes_728.jpeg          \n","  inflating: shoes_7300.jpeg         \n","  inflating: shoes_7306.jpeg         \n","  inflating: shoes_7309.jpeg         \n","  inflating: shoes_731.jpeg          \n","  inflating: shoes_7313.jpeg         \n","  inflating: shoes_7317.jpeg         \n","  inflating: shoes_7322.jpeg         \n","  inflating: shoes_7326.jpeg         \n","  inflating: shoes_7333.jpeg         \n","  inflating: shoes_7338.jpeg         \n","  inflating: shoes_7339.jpeg         \n","  inflating: shoes_7340.jpeg         \n","  inflating: shoes_7345.jpeg         \n","  inflating: shoes_7346.jpeg         \n","  inflating: shoes_7347.jpeg         \n","  inflating: shoes_7348.jpeg         \n","  inflating: shoes_7351.jpeg         \n","  inflating: shoes_7359.jpeg         \n","  inflating: shoes_736.jpeg          \n","  inflating: shoes_7372.jpeg         \n","  inflating: shoes_7383.jpeg         \n","  inflating: shoes_7384.jpeg         \n","  inflating: shoes_7388.jpeg         \n","  inflating: shoes_7390.jpeg         \n","  inflating: shoes_7391.jpeg         \n","  inflating: shoes_7396.jpeg         \n","  inflating: shoes_7405.jpeg         \n","  inflating: shoes_7408.jpeg         \n","  inflating: shoes_7410.jpeg         \n","  inflating: shoes_7412.jpeg         \n","  inflating: shoes_742.jpeg          \n","  inflating: shoes_7425.jpeg         \n","  inflating: shoes_7430.jpeg         \n","  inflating: shoes_7431.jpeg         \n","  inflating: shoes_7435.jpeg         \n","  inflating: shoes_7436.jpeg         \n","  inflating: shoes_7451.jpeg         \n","  inflating: shoes_7453.jpeg         \n","  inflating: shoes_7455.jpeg         \n","  inflating: shoes_7457.jpeg         \n","  inflating: shoes_7465.jpeg         \n","  inflating: shoes_7487.jpeg         \n","  inflating: shoes_7499.jpeg         \n","  inflating: shoes_750.jpeg          \n","  inflating: shoes_7502.jpeg         \n","  inflating: shoes_7507.jpeg         \n","  inflating: shoes_7511.jpeg         \n","  inflating: shoes_7516.jpeg         \n","  inflating: shoes_7530.jpeg         \n","  inflating: shoes_7533.jpeg         \n","  inflating: shoes_7547.jpeg         \n","  inflating: shoes_7549.jpeg         \n","  inflating: shoes_7556.jpeg         \n","  inflating: shoes_7557.jpeg         \n","  inflating: shoes_7585.jpeg         \n","  inflating: shoes_7589.jpeg         \n","  inflating: shoes_7592.jpeg         \n","  inflating: shoes_7597.jpeg         \n","  inflating: shoes_760.jpeg          \n","  inflating: shoes_7601.jpeg         \n","  inflating: shoes_7605.jpeg         \n","  inflating: shoes_7607.jpeg         \n","  inflating: shoes_7617.jpeg         \n","  inflating: shoes_7620.jpeg         \n","  inflating: shoes_7621.jpeg         \n","  inflating: shoes_7625.jpeg         \n","  inflating: shoes_7626.jpeg         \n","  inflating: shoes_7627.jpeg         \n","  inflating: shoes_7629.jpeg         \n","  inflating: shoes_763.jpeg          \n","  inflating: shoes_7643.jpeg         \n","  inflating: shoes_7647.jpeg         \n","  inflating: shoes_7666.jpeg         \n","  inflating: shoes_7673.jpeg         \n","  inflating: shoes_7682.jpeg         \n","  inflating: shoes_7683.jpeg         \n","  inflating: shoes_7684.jpeg         \n","  inflating: shoes_7686.jpeg         \n","  inflating: shoes_7701.jpeg         \n","  inflating: shoes_771.jpeg          \n","  inflating: shoes_7716.jpeg         \n","  inflating: shoes_7719.jpeg         \n","  inflating: shoes_7723.jpeg         \n","  inflating: shoes_7724.jpeg         \n","  inflating: shoes_7730.jpeg         \n","  inflating: shoes_774.jpeg          \n","  inflating: shoes_7756.jpeg         \n","  inflating: shoes_7760.jpeg         \n","  inflating: shoes_7765.jpeg         \n","  inflating: shoes_7767.jpeg         \n","  inflating: shoes_7783.jpeg         \n","  inflating: shoes_7786.jpeg         \n","  inflating: shoes_7789.jpeg         \n","  inflating: shoes_7790.jpeg         \n","  inflating: shoes_7791.jpeg         \n","  inflating: shoes_7798.jpeg         \n","  inflating: shoes_7799.jpeg         \n","  inflating: shoes_78.jpeg           \n","  inflating: shoes_7800.jpeg         \n","  inflating: shoes_7802.jpeg         \n","  inflating: shoes_7805.jpeg         \n","  inflating: shoes_781.jpeg          \n","  inflating: shoes_7811.jpeg         \n","  inflating: shoes_7815.jpeg         \n","  inflating: shoes_7824.jpeg         \n","  inflating: shoes_7826.jpeg         \n","  inflating: shoes_7833.jpeg         \n","  inflating: shoes_784.jpeg          \n","  inflating: shoes_7842.jpeg         \n","  inflating: shoes_7843.jpeg         \n","  inflating: shoes_7844.jpeg         \n","  inflating: shoes_785.jpeg          \n","  inflating: shoes_7852.jpeg         \n","  inflating: shoes_786.jpeg          \n","  inflating: shoes_7893.jpeg         \n","  inflating: shoes_7894.jpeg         \n","  inflating: shoes_7898.jpeg         \n","  inflating: shoes_79.jpeg           \n","  inflating: shoes_7916.jpeg         \n","  inflating: shoes_7921.jpeg         \n","  inflating: shoes_7929.jpeg         \n","  inflating: shoes_7931.jpeg         \n","  inflating: shoes_7934.jpeg         \n","  inflating: shoes_7938.jpeg         \n","  inflating: shoes_7939.jpeg         \n","  inflating: shoes_7942.jpeg         \n","  inflating: shoes_7943.jpeg         \n","  inflating: shoes_7945.jpeg         \n","  inflating: shoes_7946.jpeg         \n","  inflating: shoes_7950.jpeg         \n","  inflating: shoes_7957.jpeg         \n","  inflating: shoes_7961.jpeg         \n","  inflating: shoes_7973.jpeg         \n","  inflating: shoes_7974.jpeg         \n","  inflating: shoes_7975.jpeg         \n","  inflating: shoes_7977.jpeg         \n","  inflating: shoes_7979.jpeg         \n","  inflating: shoes_7984.jpeg         \n","  inflating: shoes_8004.jpeg         \n","  inflating: shoes_802.jpeg          \n","  inflating: shoes_8022.jpeg         \n","  inflating: shoes_8033.jpeg         \n","  inflating: shoes_8034.jpeg         \n","  inflating: shoes_8038.jpeg         \n","  inflating: shoes_8046.jpeg         \n","  inflating: shoes_8055.jpeg         \n","  inflating: shoes_8057.jpeg         \n","  inflating: shoes_806.jpeg          \n","  inflating: shoes_8066.jpeg         \n","  inflating: shoes_8067.jpeg         \n","  inflating: shoes_807.jpeg          \n","  inflating: shoes_8079.jpeg         \n","  inflating: shoes_8080.jpeg         \n","  inflating: shoes_8082.jpeg         \n","  inflating: shoes_8084.jpeg         \n","  inflating: shoes_8085.jpeg         \n","  inflating: shoes_8097.jpeg         \n","  inflating: shoes_8100.jpeg         \n","  inflating: shoes_811.jpeg          \n","  inflating: shoes_8110.jpeg         \n","  inflating: shoes_8113.jpeg         \n","  inflating: shoes_8120.jpeg         \n","  inflating: shoes_8127.jpeg         \n","  inflating: shoes_8138.jpeg         \n","  inflating: shoes_8142.jpeg         \n","  inflating: shoes_8143.jpeg         \n","  inflating: shoes_8145.jpeg         \n","  inflating: shoes_8149.jpeg         \n","  inflating: shoes_8182.jpeg         \n","  inflating: shoes_8184.jpeg         \n","  inflating: shoes_8185.jpeg         \n","  inflating: shoes_8187.jpeg         \n","  inflating: shoes_8194.jpeg         \n","  inflating: shoes_8198.jpeg         \n","  inflating: shoes_8199.jpeg         \n","  inflating: shoes_8209.jpeg         \n","  inflating: shoes_8213.jpeg         \n","  inflating: shoes_8215.jpeg         \n","  inflating: shoes_8221.jpeg         \n","  inflating: shoes_8231.jpeg         \n","  inflating: shoes_8236.jpeg         \n","  inflating: shoes_8241.jpeg         \n","  inflating: shoes_8244.jpeg         \n","  inflating: shoes_8262.jpeg         \n","  inflating: shoes_8266.jpeg         \n","  inflating: shoes_8269.jpeg         \n","  inflating: shoes_827.jpeg          \n","  inflating: shoes_8287.jpeg         \n","  inflating: shoes_8293.jpeg         \n","  inflating: shoes_8295.jpeg         \n","  inflating: shoes_8301.jpeg         \n","  inflating: shoes_8320.jpeg         \n","  inflating: shoes_8334.jpeg         \n","  inflating: shoes_8335.jpeg         \n","  inflating: shoes_8338.jpeg         \n","  inflating: shoes_8339.jpeg         \n","  inflating: shoes_834.jpeg          \n","  inflating: shoes_8344.jpeg         \n","  inflating: shoes_8346.jpeg         \n","  inflating: shoes_8348.jpeg         \n","  inflating: shoes_8354.jpeg         \n","  inflating: shoes_8357.jpeg         \n","  inflating: shoes_8358.jpeg         \n","  inflating: shoes_8360.jpeg         \n","  inflating: shoes_8370.jpeg         \n","  inflating: shoes_8372.jpeg         \n","  inflating: shoes_8373.jpeg         \n","  inflating: shoes_8379.jpeg         \n","  inflating: shoes_838.jpeg          \n","  inflating: shoes_8387.jpeg         \n","  inflating: shoes_839.jpeg          \n","  inflating: shoes_8391.jpeg         \n","  inflating: shoes_840.jpeg          \n","  inflating: shoes_8402.jpeg         \n","  inflating: shoes_8405.jpeg         \n","  inflating: shoes_8408.jpeg         \n","  inflating: shoes_841.jpeg          \n","  inflating: shoes_8422.jpeg         \n","  inflating: shoes_8428.jpeg         \n","  inflating: shoes_8431.jpeg         \n","  inflating: shoes_8438.jpeg         \n","  inflating: shoes_8442.jpeg         \n","  inflating: shoes_8444.jpeg         \n","  inflating: shoes_8446.jpeg         \n","  inflating: shoes_8457.jpeg         \n","  inflating: shoes_846.jpeg          \n","  inflating: shoes_8462.jpeg         \n","  inflating: shoes_8466.jpeg         \n","  inflating: shoes_8471.jpeg         \n","  inflating: shoes_8474.jpeg         \n","  inflating: shoes_849.jpeg          \n","  inflating: shoes_8491.jpeg         \n","  inflating: shoes_8495.jpeg         \n","  inflating: shoes_8496.jpeg         \n","  inflating: shoes_8497.jpeg         \n","  inflating: shoes_8501.jpeg         \n","  inflating: shoes_8503.jpeg         \n","  inflating: shoes_8508.jpeg         \n","  inflating: shoes_8510.jpeg         \n","  inflating: shoes_8512.jpeg         \n","  inflating: shoes_8520.jpeg         \n","  inflating: shoes_8523.jpeg         \n","  inflating: shoes_8533.jpeg         \n","  inflating: shoes_8550.jpeg         \n","  inflating: shoes_8551.jpeg         \n","  inflating: shoes_8552.jpeg         \n","  inflating: shoes_8559.jpeg         \n","  inflating: shoes_8561.jpeg         \n","  inflating: shoes_8576.jpeg         \n","  inflating: shoes_8578.jpeg         \n","  inflating: shoes_8585.jpeg         \n","  inflating: shoes_8586.jpeg         \n","  inflating: shoes_8587.jpeg         \n","  inflating: shoes_8590.jpeg         \n","  inflating: shoes_8592.jpeg         \n","  inflating: shoes_8594.jpeg         \n","  inflating: shoes_8596.jpeg         \n","  inflating: shoes_8599.jpeg         \n","  inflating: shoes_860.jpeg          \n","  inflating: shoes_8608.jpeg         \n","  inflating: shoes_8609.jpeg         \n","  inflating: shoes_8613.jpeg         \n","  inflating: shoes_8618.jpeg         \n","  inflating: shoes_8619.jpeg         \n","  inflating: shoes_8624.jpeg         \n","  inflating: shoes_8627.jpeg         \n","  inflating: shoes_8631.jpeg         \n","  inflating: shoes_8635.jpeg         \n","  inflating: shoes_8638.jpeg         \n","  inflating: shoes_8643.jpeg         \n","  inflating: shoes_8648.jpeg         \n","  inflating: shoes_8650.jpeg         \n","  inflating: shoes_8657.jpeg         \n","  inflating: shoes_8663.jpeg         \n","  inflating: shoes_8667.jpeg         \n","  inflating: shoes_8672.jpeg         \n","  inflating: shoes_8677.jpeg         \n","  inflating: shoes_8679.jpeg         \n","  inflating: shoes_8680.jpeg         \n","  inflating: shoes_8683.jpeg         \n","  inflating: shoes_8690.jpeg         \n","  inflating: shoes_8694.jpeg         \n","  inflating: shoes_8698.jpeg         \n","  inflating: shoes_8700.jpeg         \n","  inflating: shoes_8710.jpeg         \n","  inflating: shoes_8713.jpeg         \n","  inflating: shoes_8722.jpeg         \n","  inflating: shoes_8723.jpeg         \n","  inflating: shoes_8724.jpeg         \n","  inflating: shoes_8727.jpeg         \n","  inflating: shoes_8731.jpeg         \n","  inflating: shoes_8734.jpeg         \n","  inflating: shoes_8737.jpeg         \n","  inflating: shoes_8741.jpeg         \n","  inflating: shoes_8743.jpeg         \n","  inflating: shoes_8747.jpeg         \n","  inflating: shoes_8749.jpeg         \n","  inflating: shoes_8751.jpeg         \n","  inflating: shoes_8760.jpeg         \n","  inflating: shoes_8761.jpeg         \n","  inflating: shoes_8772.jpeg         \n","  inflating: shoes_8780.jpeg         \n","  inflating: shoes_8782.jpeg         \n","  inflating: shoes_8788.jpeg         \n","  inflating: shoes_8792.jpeg         \n","  inflating: shoes_8793.jpeg         \n","  inflating: shoes_8795.jpeg         \n","  inflating: shoes_8798.jpeg         \n","  inflating: shoes_8806.jpeg         \n","  inflating: shoes_8824.jpeg         \n","  inflating: shoes_8825.jpeg         \n","  inflating: shoes_8828.jpeg         \n","  inflating: shoes_8831.jpeg         \n","  inflating: shoes_8833.jpeg         \n","  inflating: shoes_8837.jpeg         \n","  inflating: shoes_8838.jpeg         \n","  inflating: shoes_8843.jpeg         \n","  inflating: shoes_8847.jpeg         \n","  inflating: shoes_8849.jpeg         \n","  inflating: shoes_885.jpeg          \n","  inflating: shoes_8853.jpeg         \n","  inflating: shoes_8859.jpeg         \n","  inflating: shoes_886.jpeg          \n","  inflating: shoes_8860.jpeg         \n","  inflating: shoes_8868.jpeg         \n","  inflating: shoes_8869.jpeg         \n","  inflating: shoes_8871.jpeg         \n","  inflating: shoes_8876.jpeg         \n","  inflating: shoes_8879.jpeg         \n","  inflating: shoes_8881.jpeg         \n","  inflating: shoes_8888.jpeg         \n","  inflating: shoes_8895.jpeg         \n","  inflating: shoes_8898.jpeg         \n","  inflating: shoes_8908.jpeg         \n","  inflating: shoes_8910.jpeg         \n","  inflating: shoes_8921.jpeg         \n","  inflating: shoes_8924.jpeg         \n","  inflating: shoes_8940.jpeg         \n","  inflating: shoes_8942.jpeg         \n","  inflating: shoes_8945.jpeg         \n","  inflating: shoes_8949.jpeg         \n","  inflating: shoes_8953.jpeg         \n","  inflating: shoes_8957.jpeg         \n","  inflating: shoes_8973.jpeg         \n","  inflating: shoes_8975.jpeg         \n","  inflating: shoes_898.jpeg          \n","  inflating: shoes_8997.jpeg         \n","  inflating: shoes_9000.jpeg         \n","  inflating: shoes_9002.jpeg         \n","  inflating: shoes_9005.jpeg         \n","  inflating: shoes_9006.jpeg         \n","  inflating: shoes_9016.jpeg         \n","  inflating: shoes_9022.jpeg         \n","  inflating: shoes_9025.jpeg         \n","  inflating: shoes_9036.jpeg         \n","  inflating: shoes_9037.jpeg         \n","  inflating: shoes_9041.jpeg         \n","  inflating: shoes_9052.jpeg         \n","  inflating: shoes_906.jpeg          \n","  inflating: shoes_9066.jpeg         \n","  inflating: shoes_9074.jpeg         \n","  inflating: shoes_9075.jpeg         \n","  inflating: shoes_9085.jpeg         \n","  inflating: shoes_9097.jpeg         \n","  inflating: shoes_9101.jpeg         \n","  inflating: shoes_9102.jpeg         \n","  inflating: shoes_9103.jpeg         \n","  inflating: shoes_9109.jpeg         \n","  inflating: shoes_9113.jpeg         \n","  inflating: shoes_9118.jpeg         \n","  inflating: shoes_912.jpeg          \n","  inflating: shoes_9121.jpeg         \n","  inflating: shoes_9129.jpeg         \n","  inflating: shoes_9130.jpeg         \n","  inflating: shoes_9139.jpeg         \n","  inflating: shoes_9140.jpeg         \n","  inflating: shoes_9148.jpeg         \n","  inflating: shoes_9151.jpeg         \n","  inflating: shoes_9152.jpeg         \n","  inflating: shoes_9154.jpeg         \n","  inflating: shoes_9156.jpeg         \n","  inflating: shoes_9157.jpeg         \n","  inflating: shoes_916.jpeg          \n","  inflating: shoes_9165.jpeg         \n","  inflating: shoes_9166.jpeg         \n","  inflating: shoes_917.jpeg          \n","  inflating: shoes_9173.jpeg         \n","  inflating: shoes_9178.jpeg         \n","  inflating: shoes_9179.jpeg         \n","  inflating: shoes_9207.jpeg         \n","  inflating: shoes_9210.jpeg         \n","  inflating: shoes_9214.jpeg         \n","  inflating: shoes_9217.jpeg         \n","  inflating: shoes_9220.jpeg         \n","  inflating: shoes_9221.jpeg         \n","  inflating: shoes_9224.jpeg         \n","  inflating: shoes_9228.jpeg         \n","  inflating: shoes_9232.jpeg         \n","  inflating: shoes_9240.jpeg         \n","  inflating: shoes_9245.jpeg         \n","  inflating: shoes_9246.jpeg         \n","  inflating: shoes_9258.jpeg         \n","  inflating: shoes_9260.jpeg         \n","  inflating: shoes_9272.jpeg         \n","  inflating: shoes_9273.jpeg         \n","  inflating: shoes_9275.jpeg         \n","  inflating: shoes_9278.jpeg         \n","  inflating: shoes_9283.jpeg         \n","  inflating: shoes_9292.jpeg         \n","  inflating: shoes_9293.jpeg         \n","  inflating: shoes_9294.jpeg         \n","  inflating: shoes_930.jpeg          \n","  inflating: shoes_9302.jpeg         \n","  inflating: shoes_9307.jpeg         \n","  inflating: shoes_931.jpeg          \n","  inflating: shoes_9315.jpeg         \n","  inflating: shoes_9318.jpeg         \n","  inflating: shoes_932.jpeg          \n","  inflating: shoes_9320.jpeg         \n","  inflating: shoes_9325.jpeg         \n","  inflating: shoes_9326.jpeg         \n","  inflating: shoes_9328.jpeg         \n","  inflating: shoes_933.jpeg          \n","  inflating: shoes_9337.jpeg         \n","  inflating: shoes_9339.jpeg         \n","  inflating: shoes_9354.jpeg         \n","  inflating: shoes_9362.jpeg         \n","  inflating: shoes_9363.jpeg         \n","  inflating: shoes_9366.jpeg         \n","  inflating: shoes_9371.jpeg         \n","  inflating: shoes_9385.jpeg         \n","  inflating: shoes_9386.jpeg         \n","  inflating: shoes_9387.jpeg         \n","  inflating: shoes_9392.jpeg         \n","  inflating: shoes_9394.jpeg         \n","  inflating: shoes_94.jpeg           \n","  inflating: shoes_9403.jpeg         \n","  inflating: shoes_9406.jpeg         \n","  inflating: shoes_941.jpeg          \n","  inflating: shoes_9411.jpeg         \n","  inflating: shoes_9418.jpeg         \n","  inflating: shoes_942.jpeg          \n","  inflating: shoes_9421.jpeg         \n","  inflating: shoes_9428.jpeg         \n","  inflating: shoes_9429.jpeg         \n","  inflating: shoes_9432.jpeg         \n","  inflating: shoes_9433.jpeg         \n","  inflating: shoes_944.jpeg          \n","  inflating: shoes_9440.jpeg         \n","  inflating: shoes_9441.jpeg         \n","  inflating: shoes_9443.jpeg         \n","  inflating: shoes_9446.jpeg         \n","  inflating: shoes_9448.jpeg         \n","  inflating: shoes_9449.jpeg         \n","  inflating: shoes_9456.jpeg         \n","  inflating: shoes_9457.jpeg         \n","  inflating: shoes_9463.jpeg         \n","  inflating: shoes_9465.jpeg         \n","  inflating: shoes_9467.jpeg         \n","  inflating: shoes_9469.jpeg         \n","  inflating: shoes_9473.jpeg         \n","  inflating: shoes_948.jpeg          \n","  inflating: shoes_949.jpeg          \n","  inflating: shoes_9499.jpeg         \n","  inflating: shoes_9505.jpeg         \n","  inflating: shoes_9506.jpeg         \n","  inflating: shoes_9512.jpeg         \n","  inflating: shoes_9522.jpeg         \n","  inflating: shoes_9524.jpeg         \n","  inflating: shoes_9529.jpeg         \n","  inflating: shoes_9531.jpeg         \n","  inflating: shoes_9532.jpeg         \n","  inflating: shoes_9537.jpeg         \n","  inflating: shoes_9543.jpeg         \n","  inflating: shoes_9552.jpeg         \n","  inflating: shoes_9559.jpeg         \n","  inflating: shoes_9565.jpeg         \n","  inflating: shoes_9569.jpeg         \n","  inflating: shoes_9572.jpeg         \n","  inflating: shoes_9574.jpeg         \n","  inflating: shoes_9584.jpeg         \n","  inflating: shoes_9588.jpeg         \n","  inflating: shoes_960.jpeg          \n","  inflating: shoes_9602.jpeg         \n","  inflating: shoes_9604.jpeg         \n","  inflating: shoes_9609.jpeg         \n","  inflating: shoes_9610.jpeg         \n","  inflating: shoes_9613.jpeg         \n","  inflating: shoes_9619.jpeg         \n","  inflating: shoes_9620.jpeg         \n","  inflating: shoes_9622.jpeg         \n","  inflating: shoes_9623.jpeg         \n","  inflating: shoes_9624.jpeg         \n","  inflating: shoes_9630.jpeg         \n","  inflating: shoes_9635.jpeg         \n","  inflating: shoes_9643.jpeg         \n","  inflating: shoes_9651.jpeg         \n","  inflating: shoes_9664.jpeg         \n","  inflating: shoes_967.jpeg          \n","  inflating: shoes_9670.jpeg         \n","  inflating: shoes_968.jpeg          \n","  inflating: shoes_9684.jpeg         \n","  inflating: shoes_9692.jpeg         \n","  inflating: shoes_9697.jpeg         \n","  inflating: shoes_9701.jpeg         \n","  inflating: shoes_9716.jpeg         \n","  inflating: shoes_9721.jpeg         \n","  inflating: shoes_9727.jpeg         \n","  inflating: shoes_9737.jpeg         \n","  inflating: shoes_974.jpeg          \n","  inflating: shoes_9741.jpeg         \n","  inflating: shoes_9744.jpeg         \n","  inflating: shoes_9748.jpeg         \n","  inflating: shoes_975.jpeg          \n","  inflating: shoes_9751.jpeg         \n","  inflating: shoes_9757.jpeg         \n","  inflating: shoes_9759.jpeg         \n","  inflating: shoes_9766.jpeg         \n","  inflating: shoes_9779.jpeg         \n","  inflating: shoes_980.jpeg          \n","  inflating: shoes_9803.jpeg         \n","  inflating: shoes_9812.jpeg         \n","  inflating: shoes_982.jpeg          \n","  inflating: shoes_9820.jpeg         \n","  inflating: shoes_9835.jpeg         \n","  inflating: shoes_9837.jpeg         \n","  inflating: shoes_9842.jpeg         \n","  inflating: shoes_9847.jpeg         \n","  inflating: shoes_9851.jpeg         \n","  inflating: shoes_986.jpeg          \n","  inflating: shoes_9860.jpeg         \n","  inflating: shoes_9863.jpeg         \n","  inflating: shoes_9865.jpeg         \n","  inflating: shoes_987.jpeg          \n","  inflating: shoes_9878.jpeg         \n","  inflating: shoes_9881.jpeg         \n","  inflating: shoes_9885.jpeg         \n","  inflating: shoes_9887.jpeg         \n","  inflating: shoes_9903.jpeg         \n","  inflating: shoes_9919.jpeg         \n","  inflating: shoes_992.jpeg          \n","  inflating: shoes_9924.jpeg         \n","  inflating: shoes_9932.jpeg         \n","  inflating: shoes_9934.jpeg         \n","  inflating: shoes_9941.jpeg         \n","  inflating: shoes_9946.jpeg         \n","  inflating: shoes_9948.jpeg         \n","  inflating: shoes_9957.jpeg         \n","  inflating: shoes_9960.jpeg         \n","  inflating: shoes_9965.jpeg         \n","  inflating: shoes_9968.jpeg         \n","  inflating: shoes_9971.jpeg         \n","  inflating: shoes_9974.jpeg         \n","  inflating: shoes_9979.jpeg         \n","  inflating: shoes_9988.jpeg         \n","  inflating: shoes_9990.jpeg         \n","  inflating: shoes_9992.jpeg         \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1uYLsMsLIvPK","colab_type":"code","outputId":"5d0abc91-1fc0-4363-b301-caee6748ec6c","executionInfo":{"status":"ok","timestamp":1580923955583,"user_tz":-60,"elapsed":5006,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["shapes = {}\n","shoes = os.listdir('images')\n","for shoe in shoes:\n","  img = io.imread('images/' + shoe)\n","  if img.shape not in shapes:\n","    shapes[img.shape] = 0\n","  shapes[img.shape] += 1\n","\n","  if img.shape != (592,592,3):\n","    os.remove('images/'+shoe)\n","\n","print(shapes)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["{(592, 592, 3): 667, (540, 540, 3): 59, (593, 444, 3): 41, (30, 75, 3): 28, (780, 780, 3): 10, (540, 540): 1}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9hu2seOaVwsR","colab_type":"code","colab":{}},"source":["!rm -r images/nike"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vISt3hLnJeLX","colab_type":"code","colab":{}},"source":["!mkdir images/nike"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gapuUzvTJl0q","colab_type":"code","colab":{}},"source":["!mv images/*.jpeg images/nike/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qoNl9gWP5Hqv","colab_type":"code","colab":{}},"source":["import shutil\n","from random import randint\n","\n","shoes_vector = [shoe for shoe in os.listdir('images/nike/')]\n","\n","shoes_to_maintain = ['11', '134', '135', '1351', \n","                     '1372', '1398', '1428', '1530', \n","                     '1561', '1605']\n","shoes_to_maintain = [\"shoes_\" + s + \".jpeg\" for s in shoes_to_maintain]                    \n","\n","#shoes_to_maintain = [shoes_vector[randint(0, len(shoes_vector))] for i in range(100)]\n","\n","shoes = os.listdir(\"images/nike/\")\n","for shoe in shoes:\n","  if shoe not in shoes_to_maintain:\n","    os.remove(\"images/nike/\" + shoe)\n","\n","for shoe in os.listdir(\"images/nike/\"):\n","    for i in range(0):\n","      basename = shoe[:-5]\n","      shutil.copyfile(\"images/nike/\" + shoe, \"images/nike/\" + basename + \"_\" + str(i) + \".jpeg\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4F5BqgVhKrhA","colab_type":"code","colab":{}},"source":["# DON'T USE\n","\n","class ShoesDataset(Dataset):\n","  def __init__(self, images, transform=None):\n","    if transform:\n","      self.images = [transform(im) for im in images]\n","    else:\n","      self.images = images\n","    self.transform = transform\n","  \n","  def __len__(self, ):\n","    return len(self.images)\n","  \n","  def __getitem__(self, index):\n","    return self.images[index], 0\n","\n","    # if self.transform:\n","    #   image = self.transform(image)\n","    # # swap color axis because\n","    # # numpy image: H x W x C\n","    # # torch image: C X H X W\n","    # return image\n","  \n","  def __iter__(self):\n","    worker_info = torch.utils.data.get_worker_info()\n","    if worker_info is None:  # single-process data loading, return the full iterator\n","        iter_start = 0\n","        iter_end = self.__len__()\n","    else:  # in a worker process\n","        # split workload\n","        per_worker = int(math.ceil(self.__len__() / float(worker_info.num_workers)))\n","        worker_id = worker_info.id\n","        iter_start = worker_id * per_worker\n","        iter_end = min(iter_start + per_worker, self.__len__())\n","    \n","    return iter(self.images[iter_start:iter_end])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8LhojroDy3y","colab_type":"code","colab":{}},"source":["# DON'T USE\n","\n","images = []\n","\n","for image in os.listdir('images'):\n","  images.append(Image.open('images/'+image))\n","\n","# Augment the dataset\n","for i in range(5):\n","  images = images + images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfTAb5B19QTJ","colab_type":"code","outputId":"f07140a6-cef5-48d6-df28-6f84f90e1a74","executionInfo":{"status":"ok","timestamp":1580312705456,"user_tz":-60,"elapsed":1123,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["len(images)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21344"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"code","metadata":{"id":"kVHObkXiK2hO","colab_type":"code","colab":{}},"source":["# DON'T USE\n","\n","imageSize = 64\n","# torch.from_numpy(image.transpose((2, 0, 1)))\n","\n","transform = transforms.Compose([transforms.Resize(imageSize),\n","                                transforms.CenterCrop(imageSize),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                               ])\n","\n","shoes_dataset = ShoesDataset(images, transform)\n","#img = shoes_dataset[0]\n","#img.view(img.size(0), -1).size()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HWhSKlvIx02","colab_type":"code","colab":{}},"source":["shoes_dataset = tv.datasets.ImageFolder(\n","        \"images/\",\n","        transforms.Compose([\n","            transforms.Resize((256,256)),\n","            transforms.CenterCrop(256),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfBPQo50IyP6","colab_type":"code","outputId":"25890d47-9bd5-488e-807d-2e6f4c72e200","executionInfo":{"status":"ok","timestamp":1580722665353,"user_tz":-60,"elapsed":8155,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["data_loader = get_data_loader(shoes_dataset, 500, 3)\n","for (i, batch) in enumerate(data_loader, 1):\n","  images, labels = batch\n","  #images.to(device)\n","  print(images.device)\n","  break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBKSOeSXsz_L","colab_type":"code","outputId":"772d92bb-9ac0-4bcf-bf6d-ac949fc3d8f5","executionInfo":{"status":"ok","timestamp":1580727146423,"user_tz":-60,"elapsed":7654,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["!pip install pro-gan-pth"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pro-gan-pth\n","  Downloading https://files.pythonhosted.org/packages/88/9a/bebca4f27ad7bf54b994a171a01e324a9e407aed7e4e1dcfc4256d8bef0b/pro-gan-pth-2.1.1.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pro-gan-pth) (1.17.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pro-gan-pth) (1.3.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pro-gan-pth) (0.4.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pro-gan-pth) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pro-gan-pth) (6.2.2)\n","Building wheels for collected packages: pro-gan-pth\n","  Building wheel for pro-gan-pth (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pro-gan-pth: filename=pro_gan_pth-2.1.1-cp36-none-any.whl size=15112 sha256=a4fef791a7bc5aca6ea6af6db8449009f3279584f1de2660687d9cebc8bde2df\n","  Stored in directory: /root/.cache/pip/wheels/e2/0c/b5/1581b3f4df4acb1d3c2f5335cf1a0177d0058e6ff9595774cc\n","Successfully built pro-gan-pth\n","Installing collected packages: pro-gan-pth\n","Successfully installed pro-gan-pth-2.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9HaICSIbs76-","colab_type":"code","colab":{}},"source":["import pro_gan_pytorch.PRO_GAN as pg\n","import torchvision as tv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4EuNNUN-s_63","colab_type":"code","colab":{}},"source":["# some parameters:\n","depth = 7\n","# hyper-parameters per depth (resolution)\n","num_epochs = [500, 500, 1000, 1000, 1000, 1000, 1000]\n","fade_ins = [50, 50, 50, 50, 50, 50, 50]\n","batch_sizes = [128, 128, 32, 32, 32, 64, 32]\n","latent_size = 256\n","device = 'cuda'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxK7d0m1HtNJ","colab_type":"code","colab":{}},"source":["!rm samples/*\n","!rm real_images/*"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"byq2Sk6stKmE","colab_type":"code","colab":{}},"source":["pro_gan = ConditionalProGAN(num_classes=1, depth=depth, latent_size=latent_size, device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7aWEt_6wy2P","colab_type":"code","colab":{}},"source":["# Load weights to the model\n","pro_gan.gen.load_state_dict(th.load(\"drive/My Drive/BDMA/BDRP_clone/models/medium_dataset/GAN_GEN_5.pth\", map_location=str(device)))\n","pro_gan.gen_shadow.load_state_dict(th.load(\"drive/My Drive/BDMA/BDRP_clone/models/medium_dataset/GAN_GEN_SHADOW_5.pth\", map_location=str(device)))\n","pro_gan.dis.load_state_dict(th.load(\"drive/My Drive/BDMA/BDRP_clone/models/medium_dataset/GAN_DIS_5.pth\", map_location=str(device)))\n","pro_gan.dis_optim.load_state_dict(th.load(\"drive/My Drive/BDMA/BDRP_clone/models/medium_dataset/GAN_DIS_OPTIM_5.pth\", map_location=str(device)))\n","pro_gan.gen_optim.load_state_dict(th.load(\"drive/My Drive/BDMA/BDRP_clone/models/medium_dataset/GAN_GEN_OPTIM_5.pth\", map_location=str(device)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3eu9zSfttTV1","colab_type":"code","outputId":"ad6e255a-5f8b-41e9-b94f-db477852f890","executionInfo":{"status":"error","timestamp":1580722189068,"user_tz":-60,"elapsed":371022,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["pro_gan.train(\n","    dataset=shoes_dataset,\n","    epochs=num_epochs,\n","    fade_in_percentage=fade_ins,\n","    batch_sizes=batch_sizes,\n","    feedback_factor=1,\n","    #start_depth=5\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Starting the training process ... \n","\n","\n","Currently working on Depth:  0\n","Current resolution: 4 x 4\n","\n","Epoch: 1\n","Elapsed: [0:00:10.907415]  batch: 1  d_loss: 27.789261  g_loss: -0.571550\n","Elapsed: [0:00:14.115881]  batch: 6  d_loss: 2.744414  g_loss: -0.961436\n","Time taken for epoch: 9.048 secs\n","\n","Epoch: 2\n","Elapsed: [0:00:20.679014]  batch: 1  d_loss: 366.947174  g_loss: -0.030997\n","Elapsed: [0:00:23.575215]  batch: 6  d_loss: 3.704019  g_loss: -0.437312\n","Time taken for epoch: 9.173 secs\n","\n","Epoch: 3\n","Elapsed: [0:00:30.269018]  batch: 1  d_loss: 9.559161  g_loss: -0.459916\n","Elapsed: [0:00:33.329046]  batch: 6  d_loss: 3.792039  g_loss: -0.431031\n","Time taken for epoch: 9.351 secs\n","\n","Epoch: 4\n","Elapsed: [0:00:40.200831]  batch: 1  d_loss: 8.107587  g_loss: -0.430628\n","Elapsed: [0:00:43.051284]  batch: 6  d_loss: 3.770103  g_loss: -0.408368\n","Time taken for epoch: 9.355 secs\n","\n","Epoch: 5\n","Elapsed: [0:00:50.364949]  batch: 1  d_loss: 6.123162  g_loss: -0.410173\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:00:53.704213]  batch: 6  d_loss: 4.086841  g_loss: -0.399138\n","Time taken for epoch: 10.290 secs\n","\n","Epoch: 6\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:01:00.387477]  batch: 1  d_loss: 5.533397  g_loss: -0.398935\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:01:03.388165]  batch: 6  d_loss: 3.784818  g_loss: -0.389752\n","Time taken for epoch: 9.330 secs\n","\n","Epoch: 7\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:01:10.672382]  batch: 1  d_loss: 7.320088  g_loss: -0.380959\n","Elapsed: [0:01:13.953741]  batch: 6  d_loss: 4.074584  g_loss: -0.380630\n","Time taken for epoch: 10.181 secs\n","\n","Epoch: 8\n","Elapsed: [0:01:20.109049]  batch: 1  d_loss: 6.365112  g_loss: -0.374288\n","Elapsed: [0:01:23.388643]  batch: 6  d_loss: 4.178417  g_loss: -0.362741\n","Time taken for epoch: 9.060 secs\n","\n","Epoch: 9\n","Elapsed: [0:01:29.783821]  batch: 1  d_loss: 6.727441  g_loss: -0.369842\n","Elapsed: [0:01:32.964992]  batch: 6  d_loss: 4.337204  g_loss: -0.349862\n","Time taken for epoch: 9.230 secs\n","\n","Epoch: 10\n","Elapsed: [0:01:40.713265]  batch: 1  d_loss: 5.938098  g_loss: -0.356468\n","Elapsed: [0:01:43.522590]  batch: 6  d_loss: 4.210362  g_loss: -0.349802\n","Time taken for epoch: 10.201 secs\n","\n","Epoch: 11\n","Elapsed: [0:01:50.098998]  batch: 1  d_loss: 5.612326  g_loss: -0.360359\n","Elapsed: [0:01:53.149911]  batch: 6  d_loss: 4.120222  g_loss: -0.345289\n","Time taken for epoch: 9.254 secs\n","\n","Epoch: 12\n","Elapsed: [0:01:59.811163]  batch: 1  d_loss: 5.876863  g_loss: -0.352417\n","Elapsed: [0:02:02.843925]  batch: 6  d_loss: 4.064720  g_loss: -0.355124\n","Time taken for epoch: 9.334 secs\n","\n","Epoch: 13\n","Elapsed: [0:02:10.069015]  batch: 1  d_loss: 6.149621  g_loss: -0.351217\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:02:13.032559]  batch: 6  d_loss: 4.356659  g_loss: -0.332122\n","Time taken for epoch: 9.824 secs\n","\n","Epoch: 14\n","Elapsed: [0:02:18.838654]  batch: 1  d_loss: 6.166501  g_loss: -0.344051\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Traceback (most recent call last):\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:02:22.260862]  batch: 6  d_loss: 4.275169  g_loss: -0.344082\n","Time taken for epoch: 8.865 secs\n","\n","Epoch: 15\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:02:28.662568]  batch: 1  d_loss: 5.057474  g_loss: -0.339757\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:02:31.982447]  batch: 6  d_loss: 4.592773  g_loss: -0.334870\n","Time taken for epoch: 9.358 secs\n","\n","Epoch: 16\n","Elapsed: [0:02:38.134294]  batch: 1  d_loss: 6.519692  g_loss: -0.325405\n","Elapsed: [0:02:41.590318]  batch: 6  d_loss: 4.405572  g_loss: -0.318312\n","Time taken for epoch: 9.233 secs\n","\n","Epoch: 17\n","Elapsed: [0:02:48.945899]  batch: 1  d_loss: 5.135191  g_loss: -0.318355\n","Elapsed: [0:02:52.012056]  batch: 6  d_loss: 4.288841  g_loss: -0.315374\n","Time taken for epoch: 10.107 secs\n","\n","Epoch: 18\n","Elapsed: [0:02:58.648823]  batch: 1  d_loss: 4.959408  g_loss: -0.316813\n","Elapsed: [0:03:01.616150]  batch: 6  d_loss: 4.270096  g_loss: -0.304954\n","Time taken for epoch: 9.201 secs\n","\n","Epoch: 19\n","Elapsed: [0:03:08.050722]  batch: 1  d_loss: 4.597086  g_loss: -0.300691\n","Elapsed: [0:03:11.044640]  batch: 6  d_loss: 4.022625  g_loss: -0.292172\n","Time taken for epoch: 9.074 secs\n","\n","Epoch: 20\n","Elapsed: [0:03:17.634406]  batch: 1  d_loss: 6.945044  g_loss: -0.288021\n","Elapsed: [0:03:20.555542]  batch: 6  d_loss: 4.232603  g_loss: -0.250153\n","Time taken for epoch: 9.152 secs\n","\n","Epoch: 21\n","Elapsed: [0:03:26.860259]  batch: 1  d_loss: 6.408000  g_loss: -0.303687\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:03:30.077555]  batch: 6  d_loss: 4.192676  g_loss: -0.284715\n","Time taken for epoch: 9.177 secs\n","\n","Epoch: 22\n","Elapsed: [0:03:36.446424]  batch: 1  d_loss: 9.833963  g_loss: -0.212887\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:03:39.889504]  batch: 6  d_loss: 4.365964  g_loss: -0.239963\n","Time taken for epoch: 9.439 secs\n","\n","Epoch: 23\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:03:46.898996]  batch: 1  d_loss: 6.338916  g_loss: -0.256291\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:03:50.503539]  batch: 6  d_loss: 4.286749  g_loss: -0.229554\n","Time taken for epoch: 10.214 secs\n","\n","Epoch: 24\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","AssertionError: can only join a child process\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:03:57.238625]  batch: 1  d_loss: 6.519727  g_loss: -0.263688\n","Elapsed: [0:04:00.568898]  batch: 6  d_loss: 4.295782  g_loss: -0.242106\n","Time taken for epoch: 9.706 secs\n","\n","Epoch: 25\n","Elapsed: [0:04:07.200798]  batch: 1  d_loss: 5.128938  g_loss: -0.226382\n","Elapsed: [0:04:10.062896]  batch: 6  d_loss: 4.514968  g_loss: -0.215918\n","Time taken for epoch: 9.136 secs\n","\n","Epoch: 26\n","Elapsed: [0:04:17.583280]  batch: 1  d_loss: 5.188100  g_loss: -0.241804\n","Elapsed: [0:04:20.468170]  batch: 6  d_loss: 4.712517  g_loss: -0.221079\n","Time taken for epoch: 10.016 secs\n","\n","Epoch: 27\n","Elapsed: [0:04:26.589269]  batch: 1  d_loss: 5.666033  g_loss: -0.229928\n","Elapsed: [0:04:29.809249]  batch: 6  d_loss: 4.486682  g_loss: -0.228774\n","Time taken for epoch: 8.999 secs\n","\n","Epoch: 28\n","Elapsed: [0:04:36.622939]  batch: 1  d_loss: 8.462282  g_loss: -0.178644\n","Elapsed: [0:04:39.442091]  batch: 6  d_loss: 4.347704  g_loss: -0.156651\n","Time taken for epoch: 9.252 secs\n","\n","Epoch: 29\n","Elapsed: [0:04:46.735695]  batch: 1  d_loss: 4.600376  g_loss: -0.239929\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:04:49.748453]  batch: 6  d_loss: 4.586291  g_loss: -0.201054\n","Time taken for epoch: 9.940 secs\n","\n","Epoch: 30\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:04:56.140702]  batch: 1  d_loss: 3.658842  g_loss: -0.213758\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","Traceback (most recent call last):\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","AssertionError: can only join a child process\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","AssertionError: can only join a child process\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:04:59.440871]  batch: 6  d_loss: 4.870016  g_loss: -0.212372\n","Time taken for epoch: 9.333 secs\n","\n","Epoch: 31\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:05:05.703582]  batch: 1  d_loss: 3.740767  g_loss: -0.212177\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:05:08.825102]  batch: 6  d_loss: 4.526847  g_loss: -0.195420\n","Time taken for epoch: 9.021 secs\n","\n","Epoch: 32\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:05:15.181525]  batch: 1  d_loss: 5.508488  g_loss: -0.205483\n","Elapsed: [0:05:18.291739]  batch: 6  d_loss: 4.718330  g_loss: -0.245240\n","Time taken for epoch: 9.076 secs\n","\n","Epoch: 33\n","Elapsed: [0:05:24.379605]  batch: 1  d_loss: 12.113687  g_loss: -0.091706\n","Elapsed: [0:05:27.635371]  batch: 6  d_loss: 0.704948  g_loss: 0.002520\n","Time taken for epoch: 8.960 secs\n","\n","Epoch: 34\n","Elapsed: [0:05:34.048416]  batch: 1  d_loss: 103.903496  g_loss: -0.270726\n","Elapsed: [0:05:37.353115]  batch: 6  d_loss: 4.978960  g_loss: -0.170907\n","Time taken for epoch: 9.356 secs\n","\n","Epoch: 35\n","Elapsed: [0:05:43.564133]  batch: 1  d_loss: 1.681455  g_loss: -0.172361\n","Elapsed: [0:05:46.690716]  batch: 6  d_loss: 5.285031  g_loss: -0.166493\n","Time taken for epoch: 9.004 secs\n","\n","Epoch: 36\n","Elapsed: [0:05:52.908813]  batch: 1  d_loss: 1.483371  g_loss: -0.169718\n","Elapsed: [0:05:56.312422]  batch: 6  d_loss: 5.011392  g_loss: -0.158783\n","Time taken for epoch: 9.239 secs\n","\n","Epoch: 37\n","Elapsed: [0:06:02.826656]  batch: 1  d_loss: 1.316796  g_loss: -0.171156\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcce114be0>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7efcd6763278>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n","    w.join()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"stream","text":["Elapsed: [0:06:05.896573]  batch: 6  d_loss: 5.375273  g_loss: -0.147675\n","Time taken for epoch: 9.226 secs\n","\n","Epoch: 38\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-bdbe04612906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfade_in_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfade_ins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeedback_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#start_depth=5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n","\u001b[0;32m<ipython-input-15-c67cca98a9a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, epochs, batch_sizes, fade_in_percentage, start_depth, num_workers, feedback_factor, log_dir, sample_dir, save_dir, checkpoint_factor)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# counter for number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                     \u001b[0;31m# calculate the alpha for fading in the layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfader_point\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mfader_point\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"hGkFK3xaF-4z","colab_type":"code","outputId":"d8c03752-6dee-4674-8ea1-1099fb46fb80","executionInfo":{"status":"error","timestamp":1580727313464,"user_tz":-60,"elapsed":793,"user":{"displayName":"Edoardo Conte","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mByBM3AoLffod-QiCrppkBDqOvl_Z8muKkRZzUEfA=s64","userId":"03709069412649048685"}},"colab":{"base_uri":"https://localhost:8080/","height":386}},"source":["from random import randint\n","\n","batch_size = 16\n","\n","#data = get_data_loader(shoes_dataset, batch_size, 3)\n","#for (i, batch) in enumerate(data, 1):\n","#  images, labels = batch\n","#  label_information = pro_gan.one_hot_encode(labels).to(device)\n","#  break\n","\n","labels = th.zeros(batch_size, dtype=int)\n","\n","label_information = pro_gan.one_hot_encode(labels).to(device)\n","latent_vector = th.randn(batch_size,\n","                          latent_size - 1).to(device)\n","gan_input = th.cat((label_information, latent_vector), dim=-1)\n","\n","images = pro_gan.gen(gan_input, 6, 1)\n","\n","pro_gan.create_grid(\n","    samples=images,\n","    scale_factor=1,\n","    img_file=\"drive/My Drive/BDMA/BDRP_clone/generated_images/medium_dataset/img_\" + str(randint(0, 100000))+ \".png\",\n",")\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-f14a25328b39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgan_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_information\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpro_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m pro_gan.create_grid(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-c67cca98a9a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, depth, alpha)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Requested output depth cannot be produced\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pro_gan_pytorch/CustomLayers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# perform the forward computations:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pro_gan_pytorch/CustomLayers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         return conv_transpose2d(input=x,\n\u001b[0;32m--> 100\u001b[0;31m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# scale the weight on runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                                 \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                 \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 15.16 GiB already allocated; 11.88 MiB free; 25.73 MiB cached)"]}]},{"cell_type":"code","metadata":{"id":"wxH30SPUzMuc","colab_type":"code","colab":{}},"source":["import datetime\n","import os\n","import time\n","import timeit\n","import copy\n","import numpy as np\n","import torch as th\n","\n","class Generator(th.nn.Module):\n","    \"\"\" Generator of the GAN network \"\"\"\n","\n","    def __init__(self, depth=7, latent_size=512, use_eql=True):\n","        \"\"\"\n","        constructor for the Generator class\n","        :param depth: required depth of the Network\n","        :param latent_size: size of the latent manifold\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import ModuleList\n","        from pro_gan_pytorch.CustomLayers import GenGeneralConvBlock, GenInitialBlock\n","        from torch.nn.functional import interpolate\n","\n","        super(Generator, self).__init__()\n","\n","        assert latent_size != 0 and ((latent_size & (latent_size - 1)) == 0), \\\n","            \"latent size not a power of 2\"\n","        if depth >= 4:\n","            assert latent_size >= np.power(2, depth - 4), \"latent size will diminish to zero\"\n","\n","        # state of the generator:\n","        self.use_eql = use_eql\n","        self.depth = depth\n","        self.latent_size = latent_size\n","\n","        # register the modules required for the GAN\n","        self.initial_block = GenInitialBlock(self.latent_size, use_eql=self.use_eql)\n","\n","        # create a module list of the other required general convolution blocks\n","        self.layers = ModuleList([])  # initialize to empty list\n","\n","        # create the ToRGB layers for various outputs:\n","        if self.use_eql:\n","            from pro_gan_pytorch.CustomLayers import _equalized_conv2d\n","            self.toRGB = lambda in_channels: \\\n","                _equalized_conv2d(in_channels, 3, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.toRGB = lambda in_channels: Conv2d(in_channels, 3, (1, 1), bias=True)\n","\n","        self.rgb_converters = ModuleList([self.toRGB(self.latent_size)])\n","\n","        # create the remaining layers\n","        for i in range(self.depth - 1):\n","            if i <= 2:\n","                layer = GenGeneralConvBlock(self.latent_size,\n","                                            self.latent_size, use_eql=self.use_eql)\n","                rgb = self.toRGB(self.latent_size)\n","            else:\n","                layer = GenGeneralConvBlock(\n","                    int(self.latent_size // np.power(2, i - 3)),\n","                    int(self.latent_size // np.power(2, i - 2)),\n","                    use_eql=self.use_eql\n","                )\n","                rgb = self.toRGB(int(self.latent_size // np.power(2, i - 2)))\n","            self.layers.append(layer)\n","            self.rgb_converters.append(rgb)\n","\n","        # register the temporary upsampler\n","        self.temporaryUpsampler = lambda x: interpolate(x, scale_factor=2)\n","\n","    def forward(self, x, depth, alpha):\n","        \"\"\"\n","        forward pass of the Generator\n","        :param x: input noise\n","        :param depth: current depth from where output is required\n","        :param alpha: value of alpha for fade-in effect\n","        :return: y => output\n","        \"\"\"\n","\n","        assert depth < self.depth, \"Requested output depth cannot be produced\"\n","\n","        y = self.initial_block(x)\n","\n","        if depth > 0:\n","            for block in self.layers[:depth - 1]:\n","                y = block(y)\n","\n","            residual = self.rgb_converters[depth - 1](self.temporaryUpsampler(y))\n","            straight = self.rgb_converters[depth](self.layers[depth - 1](y))\n","\n","            out = (alpha * straight) + ((1 - alpha) * residual)\n","\n","        else:\n","            out = self.rgb_converters[0](y)\n","\n","        return out\n","\n","\n","# ========================================================================================\n","# Discriminator Module\n","# can be used with ProGAN or standalone (for inference).\n","# Note this cannot be used with ConditionalProGAN\n","# ========================================================================================\n","\n","class Discriminator(th.nn.Module):\n","    \"\"\" Discriminator of the GAN \"\"\"\n","\n","    def __init__(self, height=7, feature_size=512, use_eql=True):\n","        \"\"\"\n","        constructor for the class\n","        :param height: total height of the discriminator (Must be equal to the Generator depth)\n","        :param feature_size: size of the deepest features extracted\n","                             (Must be equal to Generator latent_size)\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import ModuleList, AvgPool2d\n","        from pro_gan_pytorch.CustomLayers import DisGeneralConvBlock, DisFinalBlock\n","\n","        super(Discriminator, self).__init__()\n","\n","        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n","            \"latent size not a power of 2\"\n","        if height >= 4:\n","            assert feature_size >= np.power(2, height - 4), \"feature size cannot be produced\"\n","\n","        # create state of the object\n","        self.use_eql = use_eql\n","        self.height = height\n","        self.feature_size = feature_size\n","\n","        self.final_block = DisFinalBlock(self.feature_size, use_eql=self.use_eql)\n","\n","        # create a module list of the other required general convolution blocks\n","        self.layers = ModuleList([])  # initialize to empty list\n","\n","        # create the fromRGB layers for various inputs:\n","        if self.use_eql:\n","            from pro_gan_pytorch.CustomLayers import _equalized_conv2d\n","            self.fromRGB = lambda out_channels: \\\n","                _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n","\n","        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n","\n","        # create the remaining layers\n","        for i in range(self.height - 1):\n","            if i > 2:\n","                layer = DisGeneralConvBlock(\n","                    int(self.feature_size // np.power(2, i - 2)),\n","                    int(self.feature_size // np.power(2, i - 3)),\n","                    use_eql=self.use_eql\n","                )\n","                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n","            else:\n","                layer = DisGeneralConvBlock(self.feature_size,\n","                                            self.feature_size, use_eql=self.use_eql)\n","                rgb = self.fromRGB(self.feature_size)\n","\n","            self.layers.append(layer)\n","            self.rgb_to_features.append(rgb)\n","\n","        # register the temporary downSampler\n","        self.temporaryDownsampler = AvgPool2d(2)\n","\n","    def forward(self, x, height, alpha):\n","        \"\"\"\n","        forward pass of the discriminator\n","        :param x: input to the network\n","        :param height: current height of operation (Progressive GAN)\n","        :param alpha: current value of alpha for fade-in\n","        :return: out => raw prediction values (WGAN-GP)\n","        \"\"\"\n","\n","        assert height < self.height, \"Requested output depth cannot be produced\"\n","\n","        if height > 0:\n","            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n","\n","            straight = self.layers[height - 1](\n","                self.rgb_to_features[height](x)\n","            )\n","\n","            y = (alpha * straight) + ((1 - alpha) * residual)\n","\n","            for block in reversed(self.layers[:height - 1]):\n","                y = block(y)\n","        else:\n","            y = self.rgb_to_features[0](x)\n","\n","        out = self.final_block(y)\n","\n","        return out\n","\n","\n","# ========================================================================================\n","# ConditionalDiscriminator Module\n","# uses the projection discrimination mechanism\n","# can be used with ConditionalProGAN or standalone (for inference)\n","# Note that this is not to be used with ProGAN\n","# ========================================================================================\n","\n","class ConditionalDiscriminator(th.nn.Module):\n","    \"\"\" Discriminator of the GAN \"\"\"\n","\n","    def __init__(self, num_classes, height=7, feature_size=512, use_eql=True):\n","        \"\"\"\n","        constructor for the class\n","        :param num_classes: number of classes for conditional discrimination\n","        :param height: total height of the discriminator (Must be equal to the Generator depth)\n","        :param feature_size: size of the deepest features extracted\n","                             (Must be equal to Generator latent_size)\n","        :param use_eql: whether to use equalized learning rate\n","        \"\"\"\n","        from torch.nn import ModuleList, AvgPool2d\n","        from pro_gan_pytorch.CustomLayers import DisGeneralConvBlock, ConDisFinalBlock\n","\n","        super(ConditionalDiscriminator, self).__init__()\n","\n","        assert feature_size != 0 and ((feature_size & (feature_size - 1)) == 0), \\\n","            \"latent size not a power of 2\"\n","        if height >= 4:\n","            assert feature_size >= np.power(2, height - 4), \"feature size cannot be produced\"\n","\n","        # create state of the object\n","        self.use_eql = use_eql\n","        self.height = height\n","        self.feature_size = feature_size\n","        self.num_classes = num_classes\n","\n","        self.final_block = ConDisFinalBlock(self.feature_size, self.num_classes,\n","                                            use_eql=self.use_eql)\n","\n","        # create a module list of the other required general convolution blocks\n","        self.layers = ModuleList([])  # initialize to empty list\n","\n","        # create the fromRGB layers for various inputs:\n","        if self.use_eql:\n","            from pro_gan_pytorch.CustomLayers import _equalized_conv2d\n","            self.fromRGB = lambda out_channels: \\\n","                _equalized_conv2d(3, out_channels, (1, 1), bias=True)\n","        else:\n","            from torch.nn import Conv2d\n","            self.fromRGB = lambda out_channels: Conv2d(3, out_channels, (1, 1), bias=True)\n","\n","        self.rgb_to_features = ModuleList([self.fromRGB(self.feature_size)])\n","\n","        # create the remaining layers\n","        for i in range(self.height - 1):\n","            if i > 2:\n","                layer = DisGeneralConvBlock(\n","                    int(self.feature_size // np.power(2, i - 2)),\n","                    int(self.feature_size // np.power(2, i - 3)),\n","                    use_eql=self.use_eql\n","                )\n","                rgb = self.fromRGB(int(self.feature_size // np.power(2, i - 2)))\n","            else:\n","                layer = DisGeneralConvBlock(self.feature_size,\n","                                            self.feature_size, use_eql=self.use_eql)\n","                rgb = self.fromRGB(self.feature_size)\n","\n","            self.layers.append(layer)\n","            self.rgb_to_features.append(rgb)\n","\n","        # register the temporary downSampler\n","        self.temporaryDownsampler = AvgPool2d(2)\n","\n","    def forward(self, x, labels, height, alpha):\n","        \"\"\"\n","        forward pass of the discriminator\n","        :param x: input to the network\n","        :param labels: labels required for conditional discrimination\n","                       note that these are pure integer labels of shape [B x 1]\n","        :param height: current height of operation (Progressive GAN)\n","        :param alpha: current value of alpha for fade-in\n","        :return: out => raw prediction values\n","        \"\"\"\n","\n","        assert height < self.height, \"Requested output depth cannot be produced\"\n","\n","        if height > 0:\n","            residual = self.rgb_to_features[height - 1](self.temporaryDownsampler(x))\n","\n","            straight = self.layers[height - 1](\n","                self.rgb_to_features[height](x)\n","            )\n","\n","            y = (alpha * straight) + ((1 - alpha) * residual)\n","\n","            for block in reversed(self.layers[:height - 1]):\n","                y = block(y)\n","        else:\n","            y = self.rgb_to_features[0](x)\n","\n","        out = self.final_block(y, labels)\n","\n","        return out\n","\n","class ConditionalProGAN:\n","    \"\"\" Wrapper around the Generator and the Conditional Discriminator \"\"\"\n","\n","    def __init__(self, num_classes, depth=7, latent_size=512,\n","                 learning_rate=0.001, beta_1=0, beta_2=0.99,\n","                 eps=1e-8, drift=0.001, n_critic=1, use_eql=True,\n","                 loss=\"wgan-gp\", use_ema=True, ema_decay=0.999,\n","                 device=th.device(\"cpu\")):\n","        \"\"\"\n","        constructor for the class\n","        :param num_classes: number of classes required for the conditional gan\n","        :param depth: depth of the GAN (will be used for each generator and discriminator)\n","        :param latent_size: latent size of the manifold used by the GAN\n","        :param learning_rate: learning rate for Adam\n","        :param beta_1: beta_1 for Adam\n","        :param beta_2: beta_2 for Adam\n","        :param eps: epsilon for Adam\n","        :param n_critic: number of times to update discriminator\n","                         (Used only if loss is wgan or wgan-gp)\n","        :param drift: drift penalty for the\n","                      (Used only if loss is wgan or wgan-gp)\n","        :param use_eql: whether to use equalized learning rate\n","        :param loss: the loss function to be used\n","                     Can either be a string =>\n","                          [\"wgan-gp\", \"wgan\", \"lsgan\", \"lsgan-with-sigmoid\",\n","                          \"hinge\", \"standard-gan\" or \"relativistic-hinge\"]\n","                     Or an instance of ConditionalGANLoss\n","        :param use_ema: boolean for whether to use exponential moving averages\n","        :param ema_decay: value of mu for ema\n","        :param device: device to run the GAN on (GPU / CPU)\n","        \"\"\"\n","\n","        from torch.optim import Adam\n","        from torch.nn import DataParallel\n","\n","        # Create the Generator and the Discriminator\n","        self.gen = Generator(depth, latent_size, use_eql=use_eql).to(device)\n","        self.dis = ConditionalDiscriminator(\n","            num_classes, height=depth,\n","            feature_size=latent_size,\n","            use_eql=use_eql).to(device)\n","\n","        # if code is to be run on GPU, we can use DataParallel:\n","        if device == th.device(\"cuda\"):\n","            self.gen = DataParallel(self.gen)\n","            self.dis = DataParallel(self.dis)\n","\n","        # state of the object\n","        self.latent_size = latent_size\n","        self.depth = depth\n","        self.use_ema = use_ema\n","        self.num_classes = num_classes  # required for matching aware\n","        self.ema_decay = ema_decay\n","        self.n_critic = n_critic\n","        self.use_eql = use_eql\n","        self.device = device\n","        self.drift = drift\n","\n","        # define the optimizers for the discriminator and generator\n","        self.gen_optim = Adam(self.gen.parameters(), lr=learning_rate,\n","                              betas=(beta_1, beta_2), eps=eps)\n","\n","        self.dis_optim = Adam(self.dis.parameters(), lr=learning_rate,\n","                              betas=(beta_1, beta_2), eps=eps)\n","\n","        # define the loss function used for training the GAN\n","        self.loss = self.__setup_loss(loss)\n","\n","        # setup the ema for the generator\n","        if self.use_ema:\n","            from pro_gan_pytorch.CustomLayers import update_average\n","\n","            # create a shadow copy of the generator\n","            self.gen_shadow = copy.deepcopy(self.gen)\n","\n","            # updater function:\n","            self.ema_updater = update_average\n","\n","            # initialize the gen_shadow weights equal to the\n","            # weights of gen\n","            self.ema_updater(self.gen_shadow, self.gen, beta=0)\n","\n","    def __setup_loss(self, loss):\n","        import pro_gan_pytorch.Losses as losses\n","\n","        if isinstance(loss, str):\n","            loss = loss.lower()  # lowercase the string\n","            if loss == \"wgan\":\n","                loss = losses.CondWGAN_GP(self.dis, self.drift, use_gp=False)\n","                # note if you use just wgan, you will have to use weight clipping\n","                # in order to prevent gradient exploding\n","\n","            elif loss == \"wgan-gp\":\n","                loss = losses.CondWGAN_GP(self.dis, self.drift, use_gp=True)\n","\n","            elif loss == \"lsgan\":\n","                loss = losses.CondLSGAN(self.dis)\n","\n","            elif loss == \"lsgan-with-sigmoid\":\n","                loss = losses.CondLSGAN_SIGMOID(self.dis)\n","\n","            elif loss == \"hinge\":\n","                loss = losses.CondHingeGAN(self.dis)\n","\n","            elif loss == \"standard-gan\":\n","                loss = losses.CondStandardGAN(self.dis)\n","\n","            elif loss == \"relativistic-hinge\":\n","                loss = losses.CondRelativisticAverageHingeGAN(self.dis)\n","\n","            else:\n","                raise ValueError(\"Unknown loss function requested\")\n","\n","        elif not isinstance(loss, losses.ConditionalGANLoss):\n","            raise ValueError(\"loss is neither an instance of GANLoss nor a string\")\n","\n","        return loss\n","\n","    def __progressive_downsampling(self, real_batch, depth, alpha):\n","        \"\"\"\n","        private helper for downsampling the original images in order to facilitate the\n","        progressive growing of the layers.\n","        :param real_batch: batch of real samples\n","        :param depth: depth at which training is going on\n","        :param alpha: current value of the fader alpha\n","        :return: real_samples => modified real batch of samples\n","        \"\"\"\n","\n","        from torch.nn import AvgPool2d\n","        from torch.nn.functional import interpolate\n","\n","        # downsample the real_batch for the given depth\n","        down_sample_factor = int(np.power(2, self.depth - depth - 1))\n","        prior_downsample_factor = max(int(np.power(2, self.depth - depth)), 0)\n","\n","        ds_real_samples = AvgPool2d(down_sample_factor)(real_batch)\n","\n","        if depth > 0:\n","            prior_ds_real_samples = interpolate(AvgPool2d(prior_downsample_factor)(real_batch),\n","                                                scale_factor=2)\n","        else:\n","            prior_ds_real_samples = ds_real_samples\n","\n","        # real samples are a combination of ds_real_samples and prior_ds_real_samples\n","        real_samples = (alpha * ds_real_samples) + ((1 - alpha) * prior_ds_real_samples)\n","\n","        # return the so computed real_samples\n","        return real_samples\n","\n","    def optimize_discriminator(self, noise, real_batch, labels, depth, alpha):\n","        \"\"\"\n","        performs one step of weight update on discriminator using the batch of data\n","        :param noise: input noise of sample generation\n","        :param real_batch: real samples batch\n","        :param labels: (conditional classes) should be a list of integers\n","        :param depth: current depth of optimization\n","        :param alpha: current alpha for fade-in\n","        :return: current loss value\n","        \"\"\"\n","        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n","\n","        loss_val = 0\n","        for _ in range(self.n_critic):\n","            # generate a batch of samples\n","            fake_samples = self.gen(noise, depth, alpha).detach()\n","\n","            loss = self.loss.dis_loss(real_samples, fake_samples,\n","                                      labels, depth, alpha)\n","\n","            # optimize discriminator\n","            self.dis_optim.zero_grad()\n","            loss.backward()\n","            self.dis_optim.step()\n","\n","            loss_val += loss.item()\n","\n","        return loss_val / self.n_critic\n","\n","    def optimize_generator(self, noise, real_batch, labels, depth, alpha):\n","        \"\"\"\n","        performs one step of weight update on generator for the given batch_size\n","        :param noise: input random noise required for generating samples\n","        :param real_batch: real batch of samples (real samples)\n","        :param labels: labels for conditional discrimination\n","        :param depth: depth of the network at which optimization is done\n","        :param alpha: value of alpha for fade-in effect\n","        :return: current loss (Wasserstein estimate)\n","        \"\"\"\n","\n","        # create batch of real samples\n","        real_samples = self.__progressive_downsampling(real_batch, depth, alpha)\n","\n","        # generate fake samples:\n","        fake_samples = self.gen(noise, depth, alpha)\n","\n","        # TODO_complete:\n","        # Change this implementation for making it compatible for relativisticGAN\n","        loss = self.loss.gen_loss(real_samples, fake_samples, labels, depth, alpha)\n","\n","        # optimize the generator\n","        self.gen_optim.zero_grad()\n","        loss.backward()\n","        self.gen_optim.step()\n","\n","        # if use_ema is true, apply ema to the generator parameters\n","        if self.use_ema:\n","            self.ema_updater(self.gen_shadow, self.gen, self.ema_decay)\n","\n","        # return the loss value\n","        return loss.item()\n","\n","    @staticmethod\n","    def create_grid(samples, scale_factor, img_file):\n","        \"\"\"\n","        utility function to create a grid of GAN samples\n","        :param samples: generated samples for storing\n","        :param scale_factor: factor for upscaling the image\n","        :param img_file: name of file to write\n","        :return: None (saves a file)\n","        \"\"\"\n","        from torchvision.utils import save_image\n","        from torch.nn.functional import interpolate\n","\n","        # upsample the image\n","        if scale_factor > 1:\n","            samples = interpolate(samples, scale_factor=scale_factor)\n","\n","        # save the images:\n","        save_image(samples, img_file, nrow=int(np.sqrt(len(samples))),\n","                   normalize=True, scale_each=True)\n","\n","    @staticmethod\n","    def __save_label_info_file(label_file, labels):\n","        \"\"\"\n","        utility method for saving a file with labels\n","        :param label_file: path to the file to be written\n","        :param labels: label tensor\n","        :return: None (writes file to disk)\n","        \"\"\"\n","        # write file with the labels written one per line\n","        with open(label_file, \"w\") as fp:\n","            for label in labels:\n","                fp.write(str(label.item()) + \"\\n\")\n","\n","    def one_hot_encode(self, labels):\n","        \"\"\"\n","        utility method to one-hot encode the labels\n","        :param labels: tensor of labels (Batch)\n","        :return: enc_label: encoded one_hot label\n","        \"\"\"\n","        if not hasattr(self, \"label_oh_encoder\"):\n","            self.label_oh_encoder = th.nn.Embedding(self.num_classes, self.num_classes)\n","            self.label_oh_encoder.weight.data = th.eye(self.num_classes)\n","\n","        return self.label_oh_encoder(labels.view(-1))\n","\n","    def train(self, dataset, epochs, batch_sizes,\n","              fade_in_percentage, start_depth=0, num_workers=3, feedback_factor=100,\n","              log_dir=\"./models/\", sample_dir=\"./samples/\", save_dir=\"./models/\",\n","              checkpoint_factor=1):\n","        \"\"\"\n","        Utility method for training the ProGAN. Note that you don't have to necessarily use this\n","        you can use the optimize_generator and optimize_discriminator for your own training routine.\n","        :param dataset: object of the dataset used for training.\n","                        Note that this is not the dataloader (we create dataloader in this method\n","                        since the batch_sizes for resolutions can be different).\n","                        Get_item should return (Image, label) in that order\n","        :param epochs: list of number of epochs to train the network for every resolution\n","        :param batch_sizes: list of batch_sizes for every resolution\n","        :param fade_in_percentage: list of percentages of epochs per resolution\n","                                   used for fading in the new layer\n","                                   not used for first resolution, but dummy value still needed.\n","        :param start_depth: start training from this depth. def=0\n","        :param num_workers: number of workers for reading the data. def=3\n","        :param feedback_factor: number of logs per epoch. def=100\n","        :param log_dir: directory for saving the loss logs. def=\"./models/\"\n","        :param sample_dir: directory for saving the generated samples. def=\"./samples/\"\n","        :param checkpoint_factor: save model after these many epochs.\n","                                  Note that only one model is stored per resolution.\n","                                  during one resolution, the checkpoint will be updated (Rewritten)\n","                                  according to this factor.\n","        :param save_dir: directory for saving the models (.pth files)\n","        :return: None (Writes multiple files to disk)\n","        \"\"\"\n","        from pro_gan_pytorch.DataTools import get_data_loader\n","\n","        assert self.depth == len(batch_sizes), \"batch_sizes not compatible with depth\"\n","\n","        # turn the generator and discriminator into train mode\n","        self.gen.train()\n","        self.dis.train()\n","        if self.use_ema:\n","            self.gen_shadow.train()\n","\n","        # create a global time counter\n","        global_time = time.time()\n","\n","        # create fixed_input for debugging\n","        temp_data_loader = get_data_loader(dataset, batch_sizes[0], num_workers=3)\n","        _, fx_labels = next(iter(temp_data_loader))\n","        # reshape them properly\n","        fixed_labels = self.one_hot_encode(fx_labels.view(-1, 1)).to(self.device)\n","        fixed_input = th.randn(fixed_labels.shape[0],\n","                               self.latent_size - self.num_classes).to(self.device)\n","        fixed_input = th.cat((fixed_labels, fixed_input), dim=-1)\n","        del temp_data_loader  # delete the temp data_loader since it is not required anymore\n","\n","        os.makedirs(sample_dir, exist_ok=True)  # make sure the directory exists\n","        self.__save_label_info_file(os.path.join(sample_dir, \"labels.txt\"), fx_labels)\n","\n","        print(\"Starting the training process ... \")\n","        for current_depth in range(start_depth, self.depth):\n","\n","            print(\"\\n\\nCurrently working on Depth: \", current_depth)\n","            current_res = np.power(2, current_depth + 2)\n","            print(\"Current resolution: %d x %d\" % (current_res, current_res))\n","\n","            data = get_data_loader(dataset, batch_sizes[current_depth], num_workers)\n","            ticker = 1\n","\n","            for epoch in range(1, epochs[current_depth] + 1):\n","                start = timeit.default_timer()  # record time at the start of epoch\n","\n","                print(\"\\nEpoch: %d\" % epoch)\n","                total_batches = len(iter(data))\n","\n","                fader_point = int((fade_in_percentage[current_depth] / 100)\n","                                  * epochs[current_depth] * total_batches)\n","\n","                step = 0  # counter for number of iterations\n","\n","                for (i, batch) in enumerate(data, 1):\n","                    # calculate the alpha for fading in the layers\n","                    alpha = ticker / fader_point if ticker <= fader_point else 1\n","\n","                    # extract current batch of data for training\n","                    images, labels = batch\n","                    images = images.to(self.device)\n","                    labels = labels.view(-1, 1)\n","\n","                    # create the input to the Generator\n","                    label_information = self.one_hot_encode(labels).to(self.device)\n","                    latent_vector = th.randn(images.shape[0],\n","                                             self.latent_size - self.num_classes).to(self.device)\n","                    gan_input = th.cat((label_information, latent_vector), dim=-1)\n","\n","                    # optimize the discriminator:\n","                    dis_loss = self.optimize_discriminator(gan_input, images,\n","                                                           labels.to(self.device), current_depth, alpha)\n","\n","                    # optimize the generator:\n","                    gen_loss = self.optimize_generator(gan_input, images,\n","                                                       labels.to(self.device), current_depth, alpha)\n","                    \n","                    if i == 1 and epoch == 1:\n","                      real_samples = self.__progressive_downsampling(images, current_depth, alpha)\n","                      os.makedirs(\"./real_images/\", exist_ok=True)\n","                      real_image_file = os.path.join(\"./real_images/\", \"real_\" + str(current_depth) + \".png\")\n","                      \n","                      self.gen_optim.zero_grad()\n","                      self.dis_optim.zero_grad()\n","                      with th.no_grad():\n","                          self.create_grid(\n","                              samples=real_samples,\n","                              scale_factor=1,\n","                              img_file=real_image_file,\n","                          )\n","\n","                    # provide a loss feedback\n","                    if i % int(total_batches / feedback_factor) == 0 or i == 1:\n","                        elapsed = time.time() - global_time\n","                        elapsed = str(datetime.timedelta(seconds=elapsed))\n","                        print(\"Elapsed: [%s]  batch: %d  d_loss: %f  g_loss: %f\"\n","                              % (elapsed, i, dis_loss, gen_loss))\n","\n","                        # also write the losses to the log file:\n","                        os.makedirs(log_dir, exist_ok=True)\n","                        log_file = os.path.join(log_dir, \"loss_\" + str(current_depth) + \".log\")\n","                        with open(log_file, \"a\") as log:\n","                            log.write(str(step) + \"\\t\" + str(dis_loss) +\n","                                      \"\\t\" + str(gen_loss) + \"\\n\")\n","\n","                        # create a grid of samples and save it\n","                        os.makedirs(sample_dir, exist_ok=True)\n","                        gen_img_file = os.path.join(sample_dir, \"gen_\" + str(current_depth) +\n","                                                    \"_\" + str(epoch) + \"_\" +\n","                                                    str(i) + \".png\")\n","\n","                        # this is done to allow for more GPU space\n","                        self.gen_optim.zero_grad()\n","                        self.dis_optim.zero_grad()\n","                        with th.no_grad():\n","                            self.create_grid(\n","                                samples=self.gen(\n","                                    fixed_input,\n","                                    current_depth,\n","                                    alpha\n","                                ) if not self.use_ema\n","                                else self.gen_shadow(\n","                                    fixed_input,\n","                                    current_depth,\n","                                    alpha\n","                                ),\n","                                scale_factor=int(np.power(2, self.depth - current_depth - 1)),\n","                                img_file=gen_img_file,\n","                            )\n","\n","                    # increment the alpha ticker and the step\n","                    ticker += 1\n","                    step += 1\n","\n","                stop = timeit.default_timer()\n","                print(\"Time taken for epoch: %.3f secs\" % (stop - start))\n","\n","                if epoch % checkpoint_factor == 0 or epoch == 1 or epoch == epochs[current_depth]:\n","                    os.makedirs(save_dir, exist_ok=True)\n","                    gen_save_file = os.path.join(save_dir, \"GAN_GEN_\" + str(current_depth) + \".pth\")\n","                    dis_save_file = os.path.join(save_dir, \"GAN_DIS_\" + str(current_depth) + \".pth\")\n","                    gen_optim_save_file = os.path.join(save_dir,\n","                                                       \"GAN_GEN_OPTIM_\" + str(current_depth)\n","                                                       + \".pth\")\n","                    dis_optim_save_file = os.path.join(save_dir,\n","                                                       \"GAN_DIS_OPTIM_\" + str(current_depth)\n","                                                       + \".pth\")\n","\n","                    th.save(self.gen.state_dict(), gen_save_file)\n","                    th.save(self.dis.state_dict(), dis_save_file)\n","                    th.save(self.gen_optim.state_dict(), gen_optim_save_file)\n","                    th.save(self.dis_optim.state_dict(), dis_optim_save_file)\n","\n","                    # also save the shadow generator if use_ema is True\n","                    if self.use_ema:\n","                        gen_shadow_save_file = os.path.join(save_dir, \"GAN_GEN_SHADOW_\" +\n","                                                            str(current_depth) + \".pth\")\n","                        th.save(self.gen_shadow.state_dict(), gen_shadow_save_file)\n","\n","        # put the gen, shadow_gen and dis in eval mode\n","        self.gen.eval()\n","        self.dis.eval()\n","        if self.use_ema:\n","            self.gen_shadow.eval()\n","\n","        print(\"Training completed ...\")"],"execution_count":0,"outputs":[]}]}